{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Plot forecast maps with all available models.\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning) # not good to supress but they divide by nan are annoying\n",
    "#warnings.simplefilter(action='ignore', category=UserWarning) # https://github.com/pydata/xarray/issues/2273\n",
    "import json\n",
    "from esio import EsioData as ed\n",
    "from esio import ice_plot\n",
    "from esio import import_data\n",
    "import subprocess\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import timeit\n",
    "import collocate\n",
    "\n",
    "\n",
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=.8, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f3c245344e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Needed to save to netcdf\n",
    "# dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36135\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>67.47 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:36135' processes=8 cores=16>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(n_workers=8)\n",
    "client\n",
    "# dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dampedAnomalyTrend',\n",
       " 'gfdlsipn',\n",
       " 'yopp',\n",
       " 'ukmetofficesipn',\n",
       " 'ecmwfsipn',\n",
       " 'ecmwf',\n",
       " 'metreofr',\n",
       " 'ukmo',\n",
       " 'kma',\n",
       " 'ncep',\n",
       " 'usnavysipn',\n",
       " 'usnavyncep',\n",
       " 'rasmesrl',\n",
       " 'noaasipn',\n",
       " 'noaasipn_ext',\n",
       " 'usnavygofs',\n",
       " 'modcansipns_3',\n",
       " 'modcansipns_4',\n",
       " 'szapirosipn',\n",
       " 'awispin',\n",
       " 'nicosipn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################\n",
    "# Load in Observed and non-dynamic model Data\n",
    "#############################################################\n",
    "\n",
    "E = ed.EsioData.load()\n",
    "mod_dir = E.model_dir\n",
    "\n",
    "# Get sea ice thickness data\n",
    "# IceBridge Quick Look\n",
    "ds_IB = xr.open_mfdataset(os.path.join(E.obs_dir, 'iceBridgeQuickLook', 'sipn_nc_grid', '*.nc'), concat_dim='time', autoclose=True)\n",
    "# ds_IB.set_coords(['date','lat','lon'], inplace=True)\n",
    "# # Shift lat to -180 to 180 space\n",
    "# ds_IB['lon'] = ((ds_IB.lon+180)%360)-180\n",
    "# ds_IB.load()\n",
    "\n",
    "# Define models to plot\n",
    "models_2_plot = list(E.model.keys())\n",
    "models_2_plot = [x for x in models_2_plot if x not in ['piomas','MME','MME_NEW','uclsipn','hcmr']] # remove some models\n",
    "models_2_plot = [x for x in models_2_plot if E.icePredicted[x]] # Only predictive models\n",
    "#models_2_plot = ['usnavyncep']\n",
    "models_2_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-31T00:00:00.000000000 2018-10-17T00:00:00.000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#def Update_PanArctic_Maps():\n",
    "# Plotting Info\n",
    "runType = 'forecast'\n",
    "variables = ['hi']\n",
    "metrics_all = {'hi':['mean_25km_1km']}\n",
    "updateAll = False\n",
    "\n",
    "# Define Init Periods here, spaced by 7 days (aprox a week)\n",
    "# Now\n",
    "cd = datetime.datetime.now()\n",
    "cd = datetime.datetime(cd.year, cd.month, cd.day) # Set hour min sec to 0. \n",
    "# Hardcoded start date (makes incremental weeks always the same)\n",
    "start_t = datetime.datetime(1950, 1, 1) # datetime.datetime(1950, 1, 1)\n",
    "\n",
    "\n",
    "# Params for this plot\n",
    "Ndays = 1 # time period to aggregate maps to\n",
    "Npers = 260 # number of periods \n",
    "init_slice = np.arange(start_t, cd, datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "init_slice = init_slice[-Npers:] # Select only the last Npers of periods (weeks) since current date\n",
    "print(init_slice[0],init_slice[-1])\n",
    "print('')\n",
    "\n",
    "\n",
    "# Forecast times to plot\n",
    "# weeks = pd.to_timedelta(np.arange(0,5,1), unit='W')\n",
    "# months = pd.to_timedelta(np.arange(2,12,1), unit='M')\n",
    "# years = pd.to_timedelta(np.arange(1,2), unit='Y') - np.timedelta64(1, 'D') # need 364 not 365\n",
    "# slices = weeks.union(months).union(years).round('1d')\n",
    "\n",
    "days = pd.to_timedelta(np.arange(0,180,1), unit='D')\n",
    "\n",
    "da_slices = xr.DataArray(days, dims=('fore_time'))\n",
    "da_slices.fore_time.values.astype('timedelta64[D]')\n",
    "#print(da_slices)\n",
    "\n",
    "# Help conversion between \"week/month\" period used for figure naming and the actual forecast time delta value\n",
    "int_2_days_dict = dict(zip(np.arange(0,da_slices.size), da_slices.values))\n",
    "days_2_int_dict = {v: k for k, v in int_2_days_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#          Loop through each dynamical model              #\n",
    "###########################################################\n",
    "\n",
    "# Plot all Models\n",
    "for cmod in models_2_plot:\n",
    "    print(cmod)\n",
    "\n",
    "    # Load in Model\n",
    "    # Find only files that have current year and month in filename (speeds up loading)\n",
    "    all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*.nc') \n",
    "\n",
    "    # Check we have files \n",
    "    files = glob.glob(all_files)\n",
    "    if not files:\n",
    "        continue # Skip this model\n",
    "\n",
    "    # Get list of variablse we want to drop\n",
    "    drop_vars = [x for x in xr.open_dataset(sorted(files)[-1],autoclose=True).data_vars if x not in variables]\n",
    "    \n",
    "    # Check if we even have variable of interest\n",
    "    if 'hi' not in xr.open_dataset(sorted(files)[-1],autoclose=True).data_vars:\n",
    "        print('    Does not have SIT')\n",
    "        continue\n",
    "    \n",
    "    # Load in model   \n",
    "    ds_model_ALL = xr.open_mfdataset(sorted(files), \n",
    "                                 chunks={ 'fore_time': 1,'init_time': 1,'nj': 304, 'ni': 448},  \n",
    "                                 concat_dim='init_time', autoclose=True, \n",
    "                                 parallel=True, drop_variables=drop_vars)\n",
    "                                 # preprocess=lambda x : is_in_time_range(x)) # 'fore_time': 1, ,\n",
    "    ds_model_ALL.rename({'nj':'x', 'ni':'y'}, inplace=True)\n",
    "    \n",
    "    #print(ds_model_ALL)\n",
    "    \n",
    "    # Get Valid time\n",
    "    ds_model_ALL = import_data.get_valid_time(ds_model_ALL)\n",
    "    \n",
    "    # For each variable\n",
    "    for cvar in variables:\n",
    "\n",
    "        # For each init time period\n",
    "        for it in init_slice: \n",
    "            it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'m') # Start period for init period (it is end of period). Add 1 m because when\n",
    "            # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "            # So we need to add one day, so we don't double count.\n",
    "            #print(it_start,\"to\",it)\n",
    "            \n",
    "            # First check current model has any init times for this day\n",
    "            if ds_model_ALL.sel(init_time=slice(it_start, it)).init_time.size==0:\n",
    "                continue\n",
    "\n",
    "            for ft in da_slices.values: \n",
    "                cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "                cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "                # Get datetime64 of valid time start and end\n",
    "                valid_start = it_start + ft\n",
    "                valid_end = it + ft\n",
    "\n",
    "                # Check we have obs for this day\n",
    "                obs_day = ds_IB[cvar].where((ds_IB.time>=valid_start) & (ds_IB.time<=valid_end), drop=True)\n",
    "                if obs_day.time.size == 0:\n",
    "                    continue \n",
    "\n",
    "                # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "                for metric in metrics_all[cvar]:\n",
    "\n",
    "                    # File paths and stuff\n",
    "                    out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                    if not os.path.exists(out_metric_dir):\n",
    "                        os.makedirs(out_metric_dir) \n",
    "                        \n",
    "                    out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                    if not os.path.exists(out_init_dir):\n",
    "                        os.makedirs(out_init_dir)\n",
    "                        \n",
    "                    out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                    if not os.path.exists(out_mod_dir):\n",
    "                        os.makedirs(out_mod_dir)     \n",
    "                        \n",
    "                    out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "                    # Only update if either we are updating All or it doesn't yet exist\n",
    "                    # OR, its one of the last 3 init times \n",
    "                    if updateAll | (os.path.isfile(out_nc_file)==False):\n",
    "                        #print(\"    Updating...\")\n",
    "\n",
    "                        # Select init period and fore_time of interest\n",
    "                        ds_model = ds_model_ALL.sel(init_time=slice(it_start, it))\n",
    "                        \n",
    "                        # Check we found any init_times in range\n",
    "                        if ds_model.init_time.size==0:\n",
    "                            #print('init_time not found.')\n",
    "                            continue\n",
    "\n",
    "                        # Select var of interest (if available)\n",
    "                        if cvar in ds_model.variables:\n",
    "                            ds_model = ds_model[cvar]\n",
    "                        else:\n",
    "                            #print('cvar not found.')\n",
    "                            continue\n",
    "\n",
    "                        # Check if we have any valid times in range of target dates\n",
    "                        ds_model = ds_model.where((ds_model.valid_time>=valid_start) & (ds_model.valid_time<=valid_end), drop=True) \n",
    "                        if ds_model.fore_time.size == 0:\n",
    "                            #print(\"no fore_time found for target period.\")\n",
    "                            continue\n",
    "\n",
    "                        # Average over for_time and init_times\n",
    "                        ds_model = ds_model.mean(dim=['fore_time','init_time'])\n",
    "\n",
    "                        if metric=='mean_25km_1km': # For each obs point on this day, find nearest model cell value\n",
    "                            # Mean over ensemble memebers (if available)\n",
    "                            \n",
    "#                             model_points = collocate.collocate(obs_day.rename({'lat':'latitude','lon':'longitude'}), \n",
    "#                                                                ds_model.mean(dim='ensemble').rename({'lat':'latitude',\n",
    "#                                                                                                      'lon':'longitude'}), \n",
    "#                                                                h_sep=50) # nearest within 50 km\n",
    "                            # Rename to sipn format\n",
    "#                             model_points.rename({'latitude':'lat','longitude':'lon','var':metric}, inplace=True)\n",
    "#                             model_points.set_coords(['lat','lon','date'], inplace=True)\n",
    "\n",
    "                            ds_model = ds_model.mean(dim='ensemble')\n",
    "                            ds_model.name = metric\n",
    "\n",
    "                        else:\n",
    "                            raise ValueError('metric not implemented')\n",
    "\n",
    "                        # drop ensemble if still present\n",
    "                        if 'ensemble' in ds_model:\n",
    "                            ds_model = ds_model.drop('ensemble')\n",
    "\n",
    "                        # Add Coords info\n",
    "                        \n",
    "                        ds_model.coords['model'] = cmod\n",
    "                        ds_model.coords['init_start'] = it_start\n",
    "                        ds_model.coords['init_end'] = it\n",
    "                        ds_model.coords['valid_start'] = it_start+ft\n",
    "                        ds_model.coords['valid_end'] = it+ft\n",
    "                        ds_model.coords['fore_time'] = ft\n",
    "                        \n",
    "                        # Save to file\n",
    "                        ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "                        # Clean up for current model\n",
    "                        ds_model = None\n",
    "print(\"Done with models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################################\n",
    "# #          climatology  trend                             #\n",
    "# ###########################################################\n",
    "\n",
    "# cmod = 'climatology'\n",
    "\n",
    "# all_files = os.path.join(mod_dir,cmod,runType,'sipn_nc', str(cd.year)+'*.nc')\n",
    "# files = glob.glob(all_files)\n",
    "\n",
    "# obs_clim_model = xr.open_mfdataset(sorted(files), \n",
    "#         chunks={'time': 30, 'x': 304, 'y': 448},  \n",
    "#          concat_dim='time', autoclose=True, parallel=True)\n",
    "\n",
    "# # For each variable\n",
    "# for cvar in variables:\n",
    "\n",
    "#     # For each init time period\n",
    "#     for it in init_slice: \n",
    "#         it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "#         # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "#         # So we need to add one day, so we don't double count.\n",
    "#         print(it_start,\"to\",it)\n",
    "\n",
    "#         for ft in da_slices.values: \n",
    "\n",
    "#             cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "#             cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "#             # Get datetime64 of valid time start and end\n",
    "#             valid_start = it_start + ft\n",
    "#             valid_end = it + ft\n",
    "\n",
    "#             # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "#             for metric in metrics_all[cvar]:\n",
    "\n",
    "#                 # File paths and stuff\n",
    "#                 out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], metric)\n",
    "#                 if not os.path.exists(out_metric_dir):\n",
    "#                     os.makedirs(out_metric_dir) \n",
    "\n",
    "#                 out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "#                 if not os.path.exists(out_init_dir):\n",
    "#                     os.makedirs(out_init_dir)\n",
    "\n",
    "#                 out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "#                 if not os.path.exists(out_mod_dir):\n",
    "#                     os.makedirs(out_mod_dir)     \n",
    "\n",
    "#                 out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "#                 # Only update if either we are updating All or it doesn't yet exist\n",
    "#                 # OR, its one of the last 3 init times \n",
    "#                 if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-3:]):\n",
    "#                     #print(\"    Updating...\")\n",
    "\n",
    "#                     # Check if we have any valid times in range of target dates\n",
    "#                     ds_model = obs_clim_model[cvar].where((obs_clim_model.time>=valid_start) & (obs_clim_model.time<=valid_end), drop=True) \n",
    "#                     if 'time' in ds_model.lat.dims:\n",
    "#                         ds_model.coords['lat'] = ds_model.lat.isel(time=0).drop('time') # Drop time from lat/lon dims (not sure why?)\n",
    "\n",
    "#                     # If we have any time\n",
    "#                     if ds_model.time.size > 0:\n",
    "\n",
    "#                         # Average over time\n",
    "#                         ds_model = ds_model.mean(dim='time')\n",
    "\n",
    "#                         if metric=='mean': # Calc ensemble mean\n",
    "#                             ds_model = ds_model\n",
    "#                         elif metric=='SIP': # Calc probability\n",
    "#                             # Issue of some ensemble members having missing data\n",
    "#                             ocnmask = ds_model.notnull()\n",
    "#                             ds_model = (ds_model>=0.15).where(ocnmask)\n",
    "#                         elif metric=='anomaly': # Calc anomaly in reference to mean observed 1980-2010\n",
    "#                             # Get climatological mean\n",
    "#                             da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "#                             # Get anomaly\n",
    "#                             ds_model = ds_model - da_obs_mean\n",
    "#                             # Add back lat/long (get dropped because of round off differences)\n",
    "#                             ds_model['lat'] = da_obs_mean.lat\n",
    "#                             ds_model['lon'] = da_obs_mean.lon\n",
    "#                         else:\n",
    "#                             raise ValueError('metric not implemented')   \n",
    "\n",
    "#                         # Drop un-needed coords to match model format\n",
    "#                         if 'doy' in ds_model.coords:\n",
    "#                             ds_model = ds_model.drop(['doy'])\n",
    "#                         if 'xm' in ds_model.coords:\n",
    "#                             ds_model = ds_model.drop(['xm'])\n",
    "#                         if 'ym' in ds_model.coords:\n",
    "#                             ds_model = ds_model.drop(['ym'])\n",
    "                    \n",
    "#                         # Add Coords info\n",
    "#                         ds_model.name = metric\n",
    "#                         ds_model.coords['model'] = cmod\n",
    "#                         ds_model.coords['init_start'] = it_start\n",
    "#                         ds_model.coords['init_end'] = it\n",
    "#                         ds_model.coords['valid_start'] = it_start+ft\n",
    "#                         ds_model.coords['valid_end'] = it+ft\n",
    "#                         ds_model.coords['fore_time'] = ft\n",
    "                        \n",
    "#                         # Save to file\n",
    "#                         ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "#                         # Clean up for current model\n",
    "#                         ds_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#                               OBSERVATIONS                               #\n",
    "############################################################################\n",
    "\n",
    "cmod = 'Observed'\n",
    "\n",
    "updateAll = True # We ALWAYS want to update all observations, because each day we get new obs that can be used to evaluate forecasts from up to 12 months ago\n",
    "\n",
    "# For each variable\n",
    "for cvar in variables:\n",
    "\n",
    "    # For each init time period\n",
    "    for it in init_slice: \n",
    "        it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "        # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "        # So we need to add one day, so we don't double count.\n",
    "        print(it_start,\"to\",it)\n",
    "\n",
    "        for ft in da_slices.values: \n",
    "\n",
    "            cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "            cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "            # Get datetime64 of valid time start and end\n",
    "            valid_start = it_start + ft\n",
    "            valid_end = it + ft\n",
    "            \n",
    "            # Check we have obs for this day\n",
    "            obs_day = ds_IB[cvar].where((ds_IB.time>=valid_start) & (ds_IB.time<=valid_end), drop=True) \n",
    "            if obs_day.time.size == 0:\n",
    "                continue \n",
    "\n",
    "            # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "            for metric in metrics_all[cvar]:\n",
    "\n",
    "                # File paths and stuff\n",
    "                out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                if not os.path.exists(out_metric_dir):\n",
    "                    os.makedirs(out_metric_dir) \n",
    "\n",
    "                out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                if not os.path.exists(out_init_dir):\n",
    "                    os.makedirs(out_init_dir)\n",
    "\n",
    "                out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                if not os.path.exists(out_mod_dir):\n",
    "                    os.makedirs(out_mod_dir)     \n",
    "\n",
    "                out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "                # Only update if either we are updating All or it doesn't yet exist\n",
    "                # OR, its one of the last 3 init times \n",
    "                if updateAll | (os.path.isfile(out_nc_file)==False):\n",
    "                    #print(\"    Updating...\")\n",
    "\n",
    "                    if metric=='mean_25km_1km':\n",
    "                        obs_day.name = metric\n",
    "                        obs_day = obs_day.isel(time=0).drop('time')\n",
    "                        obs_out = obs_day.to_dataset()\n",
    "                    else:\n",
    "                        raise ValueError('Not implemented')\n",
    "\n",
    "                    # Add Coords info\n",
    "                    obs_out.coords['model'] = cmod\n",
    "                    obs_out.coords['init_start'] = it_start\n",
    "                    obs_out.coords['init_end'] = it\n",
    "                    obs_out.coords['valid_start'] = it_start+ft\n",
    "                    obs_out.coords['valid_end'] = it+ft\n",
    "                    obs_out.coords['fore_time'] = ft\n",
    "                    obs_out.rename({'nj':'x', 'ni':'y'}, inplace=True)\n",
    "\n",
    "\n",
    "                    # Write to disk\n",
    "                    obs_out.to_netcdf(out_nc_file)\n",
    "\n",
    "                    # Clean up for current model\n",
    "                    obs_out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in weekly metrics...\n",
      "    Loading mean_25km_1km ...\n",
      "    Found 112 initialization periods.\n"
     ]
    }
   ],
   "source": [
    "# Load in all data and write to Zarr\n",
    "cvar = 'hi'\n",
    "# Load in all metrics for given variable\n",
    "print(\"Loading in weekly metrics...\")\n",
    "ds_m = import_data.load_MME_by_init_end(E=E, \n",
    "                                        runType=runType, \n",
    "                                        variable=cvar, \n",
    "                                        metrics=metrics_all[cvar])\n",
    "\n",
    "# Drop models that we don't evaluate (i.e. monthly means)\n",
    "models_keep = [x for x in ds_m.model.values if x not in ['noaasipn','modcansipns_3','modcansipns_4']]\n",
    "ds_m = ds_m.sel(model=models_keep)\n",
    "# Get list of dynamical models that are not observations\n",
    "dynamical_Models = [x for x in ds_m.model.values if x not in ['Observed','climatology','dampedAnomaly','dampedAnomalyTrend']]\n",
    "# # Get list of all models\n",
    "# all_Models = [x for x in ds_m.model.values if x not in ['Observed']]\n",
    "# Add MME\n",
    "MME_avg = ds_m.sel(model=dynamical_Models).mean(dim='model') # only take mean over dynamical models\n",
    "MME_avg.coords['model'] = 'MME'\n",
    "ds_ALL = xr.concat([ds_m, MME_avg], dim='model')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (fore_time: 72, init_end: 56, model: 6, x: 304, y: 448)\n",
       "Coordinates:\n",
       "  * fore_time      (fore_time) timedelta64[ns] 0 days 1 days ... 72 days 74 days\n",
       "    lon            (x, y) float64 dask.array<shape=(304, 448), chunksize=(304, 448)>\n",
       "    lat            (x, y) float64 dask.array<shape=(304, 448), chunksize=(304, 448)>\n",
       "  * init_end       (init_end) datetime64[ns] 2018-02-01 ... 2018-04-16\n",
       "  * model          (model) object 'Observed' 'gfdlsipn' ... 'usnavyncep' 'MME'\n",
       "Dimensions without coordinates: x, y\n",
       "Data variables:\n",
       "    mean_25km_1km  (init_end, model, fore_time, x, y) float64 dask.array<shape=(56, 6, 72, 304, 448), chunksize=(8, 1, 8, 304, 448)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_ALL = ds_ALL.chunk({'fore_time': 8, 'init_end': 8, 'model': 1, 'x': 304, 'y': 448})\n",
    "ds_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to Zarr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:51184 remote=tcp://127.0.0.1:36135>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f306341a2e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to Zarr\n",
    "print(\"Saving to Zarr...\")\n",
    "ds_ALL.to_zarr('/home/disk/sipn/nicway/data/model/zarr/hi.zarr', mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(nan)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds_test = xr.open_zarr('/home/disk/sipn/nicway/data/model/zarr/hi.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 27s, sys: 43.3 s, total: 4min 11s\n",
      "Wall time: 1min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.54639463)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %time ds_ALL.mean_25km_1km.mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to nc...\n",
      "Finished updating Daily HI metrics and saved to Zar and nc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Save to Netcdf\n",
    "# print(\"Saving to nc...\")\n",
    "# ds_ALL.to_netcdf('/home/disk/sipn/nicway/data/model/nc/hi.nc', mode='w')\n",
    "\n",
    "print(\"Finished updating Daily HI metrics and saved to Zar and nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.4 esio",
   "language": "python",
   "name": "esio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
