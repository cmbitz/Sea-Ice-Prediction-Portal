{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "weekly average, compute metrics, save for all available models and obs\n",
    "redo for last two weeks each time called, takes a long time\n",
    "no plots are made\n",
    "'''\n",
    "\n",
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning) # not good to supress but they divide by nan are annoying\n",
    "#warnings.simplefilter(action='ignore', category=UserWarning) # https://github.com/pydata/xarray/issues/2273\n",
    "import json\n",
    "from esio import EsioData as ed\n",
    "from esio import ice_plot\n",
    "from esio import import_data\n",
    "import subprocess\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import timeit\n",
    "\n",
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=.8, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discovered that ukmetofficesipn march was actually some other month so \n",
    "# was not computing the metrics for march\n",
    "# so tried to force it here\n",
    "# once I get the data it should be possible to just run and get the metrics\n",
    "# for that model only\n",
    "# when done must adjust the end of cell a few down from here so that the other\n",
    "# models are done again and so not update all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x1543f55cc5f8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#client = Client()\n",
    "#client\n",
    "dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-29 00:00:00\n",
      "2019-06-02T00:00:00.000000000 2019-11-24T00:00:00.000000000\n",
      "\n",
      "<xarray.DataArray (fore_time: 52)>\n",
      "array([                0,   604800000000000,  1209600000000000,\n",
      "        1814400000000000,  2419200000000000,  3024000000000000,\n",
      "        3628800000000000,  4233600000000000,  4838400000000000,\n",
      "        5443200000000000,  6048000000000000,  6652800000000000,\n",
      "        7257600000000000,  7862400000000000,  8467200000000000,\n",
      "        9072000000000000,  9676800000000000, 10281600000000000,\n",
      "       10886400000000000, 11491200000000000, 12096000000000000,\n",
      "       12700800000000000, 13305600000000000, 13910400000000000,\n",
      "       14515200000000000, 15120000000000000, 15724800000000000,\n",
      "       16329600000000000, 16934400000000000, 17539200000000000,\n",
      "       18144000000000000, 18748800000000000, 19353600000000000,\n",
      "       19958400000000000, 20563200000000000, 21168000000000000,\n",
      "       21772800000000000, 22377600000000000, 22982400000000000,\n",
      "       23587200000000000, 24192000000000000, 24796800000000000,\n",
      "       25401600000000000, 26006400000000000, 26611200000000000,\n",
      "       27216000000000000, 27820800000000000, 28425600000000000,\n",
      "       29030400000000000, 29635200000000000, 30240000000000000,\n",
      "       30844800000000000], dtype='timedelta64[ns]')\n",
      "Coordinates:\n",
      "  * fore_time  (fore_time) timedelta64[ns] 0 days 7 days ... 350 days 357 days\n"
     ]
    }
   ],
   "source": [
    "#def Update_PanArctic_Maps():\n",
    "# Plotting Info\n",
    "runType = 'forecast'\n",
    "variables = ['sic']\n",
    "metrics_all = {'sic':['anomaly','mean','SIP'], 'hi':['mean']}\n",
    "#metrics_all = {'sic':['SIP']}\n",
    "\n",
    "cvar = 'sic'\n",
    "\n",
    "# Define Init Periods here, spaced by 7 days (aprox a week)\n",
    "# Now\n",
    "cd = datetime.datetime.now()\n",
    "cd = datetime.datetime(cd.year, cd.month, cd.day) # Set hour min sec to 0. \n",
    "\n",
    "#cd = datetime.datetime(2019, 4, 1) # Set hour min sec to 0. \n",
    "\n",
    "print(cd)\n",
    "# Hardcoded start date (makes incremental weeks always the same)\n",
    "start_t = datetime.datetime(1950, 1, 1) # datetime.datetime(1950, 1, 1)\n",
    "# Params for this plot\n",
    "Ndays = 7 # time period to aggregate maps to (default is 7)\n",
    "\n",
    "init_start_date = np.datetime64('2018-01-01') # first date we have computed metrics\n",
    "                   # only updates files if missing or if forced with updateAll=True\n",
    "#init_start_date = np.datetime64('2019-01-01') # speeds up substantially b\n",
    "updateAll = False # if update all may wish to adjust init_start_date\n",
    "\n",
    "# reduce this to spead things up \n",
    "init_start_date = np.datetime64('2019-10-01') # first date we have computed metrics\n",
    "#updateAll = True\n",
    "\n",
    "    \n",
    "init_slice = np.arange(start_t, cd, datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "# init_slice = init_slice[-Npers:] # Select only the last Npers of periods (weeks) since current date\n",
    "init_slice = init_slice[init_slice>=init_start_date] # Select only the inits after init_start_date\n",
    "\n",
    "print(init_slice[0],init_slice[-1])\n",
    "print('')\n",
    "\n",
    "# Forecast times\n",
    "weeks = pd.to_timedelta(np.arange(0,52,1), unit='W')\n",
    "#months = pd.to_timedelta(np.arange(2,12,1), unit='M')\n",
    "#years = pd.to_timedelta(np.arange(1,2), unit='Y') - np.timedelta64(1, 'D') # need 364 not 365\n",
    "#slices = weeks.union(months).union(years).round('1d')\n",
    "fore_slice = xr.DataArray(weeks, dims=('fore_time'))\n",
    "fore_slice.fore_time.values.astype('timedelta64[D]')\n",
    "print(fore_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dampedAnomalyTrend', 'gfdlsipn', 'yopp', 'ukmetofficesipn', 'ecmwfsipn', 'meteofrsipn', 'ecmwf', 'metreofr', 'ukmo', 'kma', 'ncep', 'usnavysipn', 'usnavyncep', 'rasmesrl', 'noaasipn', 'noaasipn_ext', 'usnavygofs', 'modcansipns_3', 'modcansipns_4', 'szapirosipn', 'awispin', 'nicosipn', 'fgoalssipn']\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# Load in Observed and non-dynamic model Data\n",
    "#############################################################\n",
    "\n",
    "E = ed.EsioData.load()\n",
    "mod_dir = E.model_dir\n",
    "\n",
    "# Get median ice edge by DOY\n",
    "median_ice_fill = xr.open_mfdataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'ice_edge.nc')).sic\n",
    "# Get mean sic by DOY\n",
    "mean_1980_2010_sic = xr.open_dataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'mean_1980_2010_sic.nc')).sic\n",
    "# Get average sip by DOY\n",
    "mean_1980_2010_SIP = xr.open_dataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'hist_SIP_1980_2010.nc')).sip    \n",
    "\n",
    "# Get recent observations\n",
    "ds_81 = xr.open_mfdataset(E.obs['NSIDC_0081']['sipn_nc']+'_yearly/*.nc', concat_dim='time', autoclose=True, parallel=True)#,\n",
    "\n",
    "# Define models to plot\n",
    "models_2_plot = list(E.model.keys())\n",
    "models_2_plot = [x for x in models_2_plot if x not in ['piomas','MME','MME_NEW','uclsipn','hcmr']] # remove some models\n",
    "models_2_plot = [x for x in models_2_plot if E.icePredicted[x]] # Only predictive models\n",
    "#models_2_plot = ['dampedAnomalyTrend']\n",
    "#models_2_plot = ['ukmo']\n",
    "#models_2_plot = ['ukmetofficesipn']\n",
    "#models_2_plot = ['usnavysipn', 'usnavyncep', 'rasmesrl', 'noaasipn', 'noaasipn_ext', 'usnavygofs', 'modcansipns_3', 'modcansipns_4', 'szapirosipn', 'awispin', 'nicosipn']\n",
    "#models_2_plot = ['fgoalssipn']\n",
    "print(models_2_plot)\n",
    "# climatology is done differently later in script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_in_time_range(x):\n",
    "    \n",
    "#     if x.sel(init_time=slice(time_bds[0],time_bds[1])).init_time.size>0: # We have some time in the time range\n",
    "#         return x\n",
    "#     else:\n",
    "#         return []\n",
    "# time_bds = [init_slice[0],init_slice[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-09-22T00:00:00.000000000' '2019-09-29T00:00:00.000000000'\n",
      " '2019-10-06T00:00:00.000000000' '2019-10-13T00:00:00.000000000'\n",
      " '2019-10-20T00:00:00.000000000' '2019-10-27T00:00:00.000000000'\n",
      " '2019-11-03T00:00:00.000000000' '2019-11-10T00:00:00.000000000'\n",
      " '2019-11-17T00:00:00.000000000' '2019-11-24T00:00:00.000000000']\n",
      "<xarray.DataArray (fore_time: 52)>\n",
      "array([                0,   604800000000000,  1209600000000000,\n",
      "        1814400000000000,  2419200000000000,  3024000000000000,\n",
      "        3628800000000000,  4233600000000000,  4838400000000000,\n",
      "        5443200000000000,  6048000000000000,  6652800000000000,\n",
      "        7257600000000000,  7862400000000000,  8467200000000000,\n",
      "        9072000000000000,  9676800000000000, 10281600000000000,\n",
      "       10886400000000000, 11491200000000000, 12096000000000000,\n",
      "       12700800000000000, 13305600000000000, 13910400000000000,\n",
      "       14515200000000000, 15120000000000000, 15724800000000000,\n",
      "       16329600000000000, 16934400000000000, 17539200000000000,\n",
      "       18144000000000000, 18748800000000000, 19353600000000000,\n",
      "       19958400000000000, 20563200000000000, 21168000000000000,\n",
      "       21772800000000000, 22377600000000000, 22982400000000000,\n",
      "       23587200000000000, 24192000000000000, 24796800000000000,\n",
      "       25401600000000000, 26006400000000000, 26611200000000000,\n",
      "       27216000000000000, 27820800000000000, 28425600000000000,\n",
      "       29030400000000000, 29635200000000000, 30240000000000000,\n",
      "       30844800000000000], dtype='timedelta64[ns]')\n",
      "Coordinates:\n",
      "  * fore_time  (fore_time) timedelta64[ns] 0 days 7 days ... 350 days 357 days\n"
     ]
    }
   ],
   "source": [
    "# will consider if we want to update all of these init times\n",
    "#init_start_date = np.datetime64('2019-09-22') # first date we have computed metrics\n",
    "#init_slice = init_slice[init_slice>=init_start_date] # Select only the inits after init_start_date\n",
    "\n",
    "print(init_slice )\n",
    "\n",
    "print(fore_slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/data/model/meteofrsipn/forecast/sipn_nc/*.nc\n",
      "\n",
      " it_start   2019-09-16T00:00:00.000000000 to 2019-09-22T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n",
      "\n",
      " it_start   2019-09-23T00:00:00.000000000 to 2019-09-29T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n",
      "\n",
      " it_start   2019-09-30T00:00:00.000000000 to 2019-10-06T00:00:00.000000000\n",
      "possible init times for model  meteofrsipn are  ['2019-10-01T00:00:00.000000000']\n",
      "actual init times for model  meteofrsipn are  ['2019-10-01T00:00:00.000000000']\n",
      "**** JUST BEFORE ft loop, ds_model_ALL is  <xarray.DataArray 'sic' (init_time: 1, ensemble: 51, fore_time: 211, x: 304, y: 448)>\n",
      "dask.array<shape=(1, 51, 211, 304, 448), dtype=float64, chunksize=(1, 51, 1, 304, 448)>\n",
      "Coordinates:\n",
      "    lon         (x, y) float64 168.3 168.4 168.5 168.7 ... -9.745 -9.872 -9.999\n",
      "    lat         (x, y) float64 31.1 31.25 31.4 31.55 ... 34.92 34.77 34.62 34.47\n",
      "  * ensemble    (ensemble) int32 0 1 2 3 4 5 6 7 8 ... 43 44 45 46 47 48 49 50\n",
      "  * fore_time   (fore_time) timedelta64[ns] 1 days 2 days ... 210 days 211 days\n",
      "  * init_time   (init_time) datetime64[ns] 2019-10-01\n",
      "    valid_time  (init_time, fore_time) datetime64[ns] 2019-10-02 ... 2020-04-29\n",
      "Dimensions without coordinates: x, y\n",
      "Attributes:\n",
      "    regrid_method:  nearest_s2d\n",
      "valid_times are  2019-09-30T00:00:00.000000000 2019-10-06T00:00:00.000000000\n",
      "valid_times are  2019-10-07T00:00:00.000000000 2019-10-13T00:00:00.000000000\n",
      "valid_times are  2019-10-14T00:00:00.000000000 2019-10-20T00:00:00.000000000\n",
      "valid_times are  2019-10-21T00:00:00.000000000 2019-10-27T00:00:00.000000000\n",
      "valid_times are  2019-10-28T00:00:00.000000000 2019-11-03T00:00:00.000000000\n",
      "valid_times are  2019-11-04T00:00:00.000000000 2019-11-10T00:00:00.000000000\n",
      "valid_times are  2019-11-11T00:00:00.000000000 2019-11-17T00:00:00.000000000\n",
      "valid_times are  2019-11-18T00:00:00.000000000 2019-11-24T00:00:00.000000000\n",
      "valid_times are  2019-11-25T00:00:00.000000000 2019-12-01T00:00:00.000000000\n",
      "valid_times are  2019-12-02T00:00:00.000000000 2019-12-08T00:00:00.000000000\n",
      "valid_times are  2019-12-09T00:00:00.000000000 2019-12-15T00:00:00.000000000\n",
      "valid_times are  2019-12-16T00:00:00.000000000 2019-12-22T00:00:00.000000000\n",
      "valid_times are  2019-12-23T00:00:00.000000000 2019-12-29T00:00:00.000000000\n",
      "valid_times are  2019-12-30T00:00:00.000000000 2020-01-05T00:00:00.000000000\n",
      "Warning Metric is Undefined, no file written:  /home/disk/sipn/nicway/data/model/MME_NEW/forecast/sipn_nc/sic/anomaly/2019-10-06/meteofrsipn/2020-01-05_meteofrsipn.nc\n",
      "valid_times are  2020-01-06T00:00:00.000000000 2020-01-12T00:00:00.000000000\n",
      "valid_times are  2020-01-13T00:00:00.000000000 2020-01-19T00:00:00.000000000\n",
      "valid_times are  2020-01-20T00:00:00.000000000 2020-01-26T00:00:00.000000000\n",
      "valid_times are  2020-01-27T00:00:00.000000000 2020-02-02T00:00:00.000000000\n",
      "valid_times are  2020-02-03T00:00:00.000000000 2020-02-09T00:00:00.000000000\n",
      "valid_times are  2020-02-10T00:00:00.000000000 2020-02-16T00:00:00.000000000\n",
      "valid_times are  2020-02-17T00:00:00.000000000 2020-02-23T00:00:00.000000000\n",
      "valid_times are  2020-02-24T00:00:00.000000000 2020-03-01T00:00:00.000000000\n",
      "valid_times are  2020-03-02T00:00:00.000000000 2020-03-08T00:00:00.000000000\n",
      "valid_times are  2020-03-09T00:00:00.000000000 2020-03-15T00:00:00.000000000\n",
      "valid_times are  2020-03-16T00:00:00.000000000 2020-03-22T00:00:00.000000000\n",
      "valid_times are  2020-03-23T00:00:00.000000000 2020-03-29T00:00:00.000000000\n",
      "valid_times are  2020-03-30T00:00:00.000000000 2020-04-05T00:00:00.000000000\n",
      "valid_times are  2020-04-06T00:00:00.000000000 2020-04-12T00:00:00.000000000\n",
      "valid_times are  2020-04-13T00:00:00.000000000 2020-04-19T00:00:00.000000000\n",
      "valid_times are  2020-04-20T00:00:00.000000000 2020-04-26T00:00:00.000000000\n",
      "valid_times are  2020-04-27T00:00:00.000000000 2020-05-03T00:00:00.000000000\n",
      "valid_times are  2020-05-04T00:00:00.000000000 2020-05-10T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-05-11T00:00:00.000000000 2020-05-17T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-05-18T00:00:00.000000000 2020-05-24T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-05-25T00:00:00.000000000 2020-05-31T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-01T00:00:00.000000000 2020-06-07T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-08T00:00:00.000000000 2020-06-14T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-15T00:00:00.000000000 2020-06-21T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-22T00:00:00.000000000 2020-06-28T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-29T00:00:00.000000000 2020-07-05T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-06T00:00:00.000000000 2020-07-12T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-13T00:00:00.000000000 2020-07-19T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-20T00:00:00.000000000 2020-07-26T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-27T00:00:00.000000000 2020-08-02T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-03T00:00:00.000000000 2020-08-09T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-10T00:00:00.000000000 2020-08-16T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-17T00:00:00.000000000 2020-08-23T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-24T00:00:00.000000000 2020-08-30T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-31T00:00:00.000000000 2020-09-06T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-09-07T00:00:00.000000000 2020-09-13T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-09-14T00:00:00.000000000 2020-09-20T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-09-21T00:00:00.000000000 2020-09-27T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "\n",
      " it_start   2019-10-07T00:00:00.000000000 to 2019-10-13T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n",
      "\n",
      " it_start   2019-10-14T00:00:00.000000000 to 2019-10-20T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n",
      "\n",
      " it_start   2019-10-21T00:00:00.000000000 to 2019-10-27T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n",
      "\n",
      " it_start   2019-10-28T00:00:00.000000000 to 2019-11-03T00:00:00.000000000\n",
      "possible init times for model  meteofrsipn are  ['2019-11-01T00:00:00.000000000']\n",
      "actual init times for model  meteofrsipn are  ['2019-11-01T00:00:00.000000000']\n",
      "**** JUST BEFORE ft loop, ds_model_ALL is  <xarray.DataArray 'sic' (init_time: 1, ensemble: 51, fore_time: 211, x: 304, y: 448)>\n",
      "dask.array<shape=(1, 51, 211, 304, 448), dtype=float64, chunksize=(1, 51, 1, 304, 448)>\n",
      "Coordinates:\n",
      "    lon         (x, y) float64 168.3 168.4 168.5 168.7 ... -9.745 -9.872 -9.999\n",
      "    lat         (x, y) float64 31.1 31.25 31.4 31.55 ... 34.92 34.77 34.62 34.47\n",
      "  * ensemble    (ensemble) int32 0 1 2 3 4 5 6 7 8 ... 43 44 45 46 47 48 49 50\n",
      "  * fore_time   (fore_time) timedelta64[ns] 1 days 2 days ... 210 days 211 days\n",
      "  * init_time   (init_time) datetime64[ns] 2019-11-01\n",
      "    valid_time  (init_time, fore_time) datetime64[ns] 2019-11-02 ... 2020-05-30\n",
      "Dimensions without coordinates: x, y\n",
      "Attributes:\n",
      "    regrid_method:  nearest_s2d\n",
      "valid_times are  2019-10-28T00:00:00.000000000 2019-11-03T00:00:00.000000000\n",
      "valid_times are  2019-11-04T00:00:00.000000000 2019-11-10T00:00:00.000000000\n",
      "valid_times are  2019-11-11T00:00:00.000000000 2019-11-17T00:00:00.000000000\n",
      "valid_times are  2019-11-18T00:00:00.000000000 2019-11-24T00:00:00.000000000\n",
      "valid_times are  2019-11-25T00:00:00.000000000 2019-12-01T00:00:00.000000000\n",
      "valid_times are  2019-12-02T00:00:00.000000000 2019-12-08T00:00:00.000000000\n",
      "valid_times are  2019-12-09T00:00:00.000000000 2019-12-15T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_times are  2019-12-16T00:00:00.000000000 2019-12-22T00:00:00.000000000\n",
      "valid_times are  2019-12-23T00:00:00.000000000 2019-12-29T00:00:00.000000000\n",
      "valid_times are  2019-12-30T00:00:00.000000000 2020-01-05T00:00:00.000000000\n",
      "Warning Metric is Undefined, no file written:  /home/disk/sipn/nicway/data/model/MME_NEW/forecast/sipn_nc/sic/anomaly/2019-11-03/meteofrsipn/2020-01-05_meteofrsipn.nc\n",
      "valid_times are  2020-01-06T00:00:00.000000000 2020-01-12T00:00:00.000000000\n",
      "valid_times are  2020-01-13T00:00:00.000000000 2020-01-19T00:00:00.000000000\n",
      "valid_times are  2020-01-20T00:00:00.000000000 2020-01-26T00:00:00.000000000\n",
      "valid_times are  2020-01-27T00:00:00.000000000 2020-02-02T00:00:00.000000000\n",
      "valid_times are  2020-02-03T00:00:00.000000000 2020-02-09T00:00:00.000000000\n",
      "valid_times are  2020-02-10T00:00:00.000000000 2020-02-16T00:00:00.000000000\n",
      "valid_times are  2020-02-17T00:00:00.000000000 2020-02-23T00:00:00.000000000\n",
      "valid_times are  2020-02-24T00:00:00.000000000 2020-03-01T00:00:00.000000000\n",
      "valid_times are  2020-03-02T00:00:00.000000000 2020-03-08T00:00:00.000000000\n",
      "valid_times are  2020-03-09T00:00:00.000000000 2020-03-15T00:00:00.000000000\n",
      "valid_times are  2020-03-16T00:00:00.000000000 2020-03-22T00:00:00.000000000\n",
      "valid_times are  2020-03-23T00:00:00.000000000 2020-03-29T00:00:00.000000000\n",
      "valid_times are  2020-03-30T00:00:00.000000000 2020-04-05T00:00:00.000000000\n",
      "valid_times are  2020-04-06T00:00:00.000000000 2020-04-12T00:00:00.000000000\n",
      "valid_times are  2020-04-13T00:00:00.000000000 2020-04-19T00:00:00.000000000\n",
      "valid_times are  2020-04-20T00:00:00.000000000 2020-04-26T00:00:00.000000000\n",
      "valid_times are  2020-04-27T00:00:00.000000000 2020-05-03T00:00:00.000000000\n",
      "valid_times are  2020-05-04T00:00:00.000000000 2020-05-10T00:00:00.000000000\n",
      "valid_times are  2020-05-11T00:00:00.000000000 2020-05-17T00:00:00.000000000\n",
      "valid_times are  2020-05-18T00:00:00.000000000 2020-05-24T00:00:00.000000000\n",
      "valid_times are  2020-05-25T00:00:00.000000000 2020-05-31T00:00:00.000000000\n",
      "valid_times are  2020-06-01T00:00:00.000000000 2020-06-07T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-08T00:00:00.000000000 2020-06-14T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-15T00:00:00.000000000 2020-06-21T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-22T00:00:00.000000000 2020-06-28T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-06-29T00:00:00.000000000 2020-07-05T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-06T00:00:00.000000000 2020-07-12T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-13T00:00:00.000000000 2020-07-19T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-20T00:00:00.000000000 2020-07-26T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-07-27T00:00:00.000000000 2020-08-02T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-03T00:00:00.000000000 2020-08-09T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-10T00:00:00.000000000 2020-08-16T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-17T00:00:00.000000000 2020-08-23T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-24T00:00:00.000000000 2020-08-30T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-08-31T00:00:00.000000000 2020-09-06T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-09-07T00:00:00.000000000 2020-09-13T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-09-14T00:00:00.000000000 2020-09-20T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-09-21T00:00:00.000000000 2020-09-27T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-09-28T00:00:00.000000000 2020-10-04T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-10-05T00:00:00.000000000 2020-10-11T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-10-12T00:00:00.000000000 2020-10-18T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "valid_times are  2020-10-19T00:00:00.000000000 2020-10-25T00:00:00.000000000\n",
      "no fore_time found for target period.\n",
      "\n",
      " it_start   2019-11-04T00:00:00.000000000 to 2019-11-10T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n",
      "\n",
      " it_start   2019-11-11T00:00:00.000000000 to 2019-11-17T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n",
      "\n",
      " it_start   2019-11-18T00:00:00.000000000 to 2019-11-24T00:00:00.000000000\n",
      "No init_times found in range for model  meteofrsipn\n"
     ]
    }
   ],
   "source": [
    "updateAll = True\n",
    "updateAll = False\n",
    "\n",
    "#models_2_plot=['fgoalssipn']\n",
    "#models_2_plot=['meteofrsipn']\n",
    "#updateAll = True\n",
    "\n",
    "\n",
    "cvar = 'sic'\n",
    "\n",
    "# Plot all Models\n",
    "for cmod in models_2_plot:\n",
    "\n",
    "    all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*.nc') \n",
    "    print(all_files)\n",
    "\n",
    "    # Check we have files \n",
    "    files = glob.glob(all_files)\n",
    "    if not files:\n",
    "        continue # Skip this model\n",
    "\n",
    "    # only want to open files in the init_slice, do not want to use dask though \n",
    "    # for the idiotic cases \n",
    "        \n",
    "    for it in init_slice: \n",
    "        it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "        # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "        # So we need to add one day, so we don't double count.\n",
    "        print('\\n it_start  ',it_start,\"to\",it)\n",
    "\n",
    "        # not actually using this but may want it later\n",
    "        tmp = str(it_start.astype('datetime64[D]')).split('-')\n",
    "        year0=tmp[0]\n",
    "        month0=tmp[1]\n",
    "        day0=tmp[2]\n",
    "        tmp=str(it.astype('datetime64[D]')).split('-')\n",
    "        year1=tmp[0]\n",
    "        month1=tmp[1]\n",
    "        day1=tmp[2]\n",
    "        #print(year0,month0,day0, ' to ' ,year1,month1,day1)\n",
    "\n",
    "        good_files = []\n",
    "        for cfile in files:\n",
    "        \n",
    "            ds=xr.open_dataset(cfile,autoclose=True).init_time\n",
    "            its = ds.values\n",
    "\n",
    "            # Check if init_times are in range (works for list or single)\n",
    "            dsinit =((its>=it_start) & (its<=it))\n",
    "        \n",
    "            if any(dsinit):\n",
    "                good_files.append(cfile)\n",
    "                \n",
    "        if not good_files:\n",
    "            print('No init_times found in range for model ',cmod)\n",
    "            continue # Skip this init_time for this model\n",
    "\n",
    "        #print('The files in range of interest are ',sorted(good_files))\n",
    "        \n",
    "        # Load in model for this init_time range, could include a few times not needed for \n",
    "        # models lumped by month\n",
    "        drop_vars = [x for x in xr.open_dataset(sorted(good_files)[-1],autoclose=True).data_vars if x not in variables]\n",
    "        ds_model_ALL = xr.open_mfdataset(sorted(good_files), \n",
    "                                     chunks={ 'fore_time': 1,'init_time': 1,'nj': 304, 'ni': 448},  \n",
    "                                     concat_dim='init_time', autoclose=True, \n",
    "                                     parallel=True, drop_variables=drop_vars)\n",
    "                                     # preprocess=lambda x : is_in_time_range(x)) # 'fore_time': 1, ,\n",
    "        ds_model_ALL.rename({'nj':'x', 'ni':'y'}, inplace=True)\n",
    "\n",
    "        # Sort by init_time\n",
    "        ds_model_ALL = ds_model_ALL.sortby('init_time')\n",
    "\n",
    "        # Get Valid time\n",
    "        ds_model_ALL = import_data.get_valid_time(ds_model_ALL)\n",
    "        print('possible init times for model ', cmod, 'are ',ds_model_ALL.init_time.values)\n",
    "\n",
    "        # Select init period and fore_time of interest\n",
    "        ds_model_ALL = ds_model_ALL.sel(init_time=slice(it_start, it))\n",
    "\n",
    "        # Check we found any init_times in range\n",
    "        if ds_model_ALL.init_time.size==0:\n",
    "            print('init_time not found.')\n",
    "            continue\n",
    "            \n",
    "        print('actual init times for model ', cmod, 'are ',ds_model_ALL.init_time.values)\n",
    "\n",
    "        # Select var of interest (if available)\n",
    "        if cvar in ds_model_ALL.variables:\n",
    "            ds_model_ALL = ds_model_ALL[cvar]\n",
    "        else:\n",
    "            print('cvar not found.')\n",
    "            continue\n",
    "\n",
    "        print('**** JUST BEFORE ft loop, ds_model_ALL is ', ds_model_ALL)\n",
    "        \n",
    "        for ft in fore_slice.values: \n",
    "\n",
    "            cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "            cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "            # Get datetime64 of valid time start and end\n",
    "            valid_start = it_start + ft\n",
    "            valid_end = it + ft\n",
    "            \n",
    "            # Check if we have any valid times in range of target dates\n",
    "            ds_model = ds_model_ALL.where((ds_model_ALL.valid_time>=valid_start) & (ds_model_ALL.valid_time<=valid_end), drop=True) \n",
    "            print('valid_times are ', valid_start, valid_end)\n",
    "            #print('ds_model ', ds_model)\n",
    "\n",
    "            if ds_model.fore_time.size == 0:\n",
    "                print(\"no fore_time found for target period.\")\n",
    "                continue\n",
    "\n",
    "            ds_avg = None\n",
    "            avg_done = False  # no avg yet\n",
    "            \n",
    "            # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "            for metric in metrics_all[cvar]:\n",
    "\n",
    "                # File paths and stuff\n",
    "                out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                if not os.path.exists(out_metric_dir):\n",
    "                    os.makedirs(out_metric_dir) \n",
    "\n",
    "                out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                if not os.path.exists(out_init_dir):\n",
    "                    os.makedirs(out_init_dir)\n",
    "\n",
    "                out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                if not os.path.exists(out_mod_dir):\n",
    "                    os.makedirs(out_mod_dir)     \n",
    "\n",
    "                out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "#                    print((os.path.isfile(out_nc_file)), out_nc_file)\n",
    "\n",
    "                # Only update if either we are updating All or it doesn't yet exist\n",
    "                # or if one of the most recent init_times \n",
    "                if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-2:]):\n",
    "                    #print(\"    Updating...\")\n",
    "   \n",
    "                    if not avg_done:\n",
    "                        ds_avg = ds_model.mean(dim=['fore_time','init_time'])\n",
    "                        #print('Got the average for computing metrics: ',metric)\n",
    "                        avg_done = True\n",
    "\n",
    "                    if metric=='mean': # Calc ensemble mean\n",
    "                        ds_metric = ds_avg.mean(dim='ensemble')\n",
    "\n",
    "                    elif metric=='SIP': # Calc probability\n",
    "                        # Remove ensemble members having missing data\n",
    "                        ok_ens = ((ds_avg.notnull().sum(dim='x').sum(dim='y'))>0) # select ensemble members with any data\n",
    "                        ds_metric = ((ds_avg.where(ok_ens, drop=True)>=0.15) ).mean(dim='ensemble').where(ds_avg.isel(ensemble=0).notnull())\n",
    "\n",
    "                    elif metric=='anomaly': # Calc anomaly in reference to mean observed 1980-2010\n",
    "                        # Get climatological mean\n",
    "                        da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "                        # Calc anomaly\n",
    "                        ds_metric = ds_avg.mean(dim='ensemble') - da_obs_mean\n",
    "                        # Add back lat/long (get dropped because of round off differences)\n",
    "                        ds_metric['lat'] = da_obs_mean.lat\n",
    "                        ds_metric['lon'] = da_obs_mean.lon\n",
    "                        \n",
    "                    else:\n",
    "                        raise ValueError('metric not implemented')\n",
    "\n",
    "                    # drop ensemble if still present does not do it though\n",
    "                    if 'ensemble' in ds_metric:\n",
    "                        print('Getting rid of ensemble variable')\n",
    "                        ds_metric = ds_metric.drop('ensemble')\n",
    "\n",
    "                    ds_metric.coords['model'] = cmod\n",
    "                    if 'xm' in ds_metric:\n",
    "                        ds_metric = ds_metric.drop(['xm','ym']) #Dump coords we don't use\n",
    "\n",
    "                    # Add Coords info\n",
    "                    ds_metric.name = metric\n",
    "                    ds_metric.coords['model'] = cmod\n",
    "                    ds_metric.coords['init_start'] = it_start\n",
    "                    ds_metric.coords['init_end'] = it\n",
    "                    ds_metric.coords['valid_start'] = it_start+ft\n",
    "                    ds_metric.coords['valid_end'] = it+ft\n",
    "                    ds_metric.coords['fore_time'] = ft\n",
    "\n",
    "                    # check if the data are missing \n",
    "                    if ds_metric.sum(dim=['x','y']).values == 0.:\n",
    "                        print('Warning Metric is Undefined, no file written: ',out_nc_file)\n",
    "                        if os.path.exists(out_nc_file):\n",
    "                            os.remove(out_nc_file)\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    #print('Done computing metric, our da is now: ', ds_metric)\n",
    "                    #print('Saving to file: ',out_nc_file)\n",
    "                    # Save to file\n",
    "                    ds_metric.to_netcdf(out_nc_file)\n",
    "\n",
    "                    #ds_metric.plot()\n",
    "                    #plt.figure()\n",
    "\n",
    "\n",
    "            #if avg_done:\n",
    "            #    diehere\n",
    "\n",
    "# Clean up \n",
    "ds_model = None\n",
    "ds_avg = None\n",
    "ds_metric = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "cfile='/home/disk/sipn/nicway/data/model/MME_NEW/forecast/sipn_nc/sic/anomaly/2019-08-25/fgoalssipn/2019-08-25_fgoalssipn.nc'\n",
    "#cfile='/home/disk/sipn/nicway/data/model/MME_NEW/forecast/sipn_nc/sic/anomaly/2019-07-28/fgoalssipn/2019-12-01_fgoalssipn.nc'\n",
    "cfile='/home/disk/sipn/nicway/data/model/MME_NEW/forecast/sipn_nc/sic/anomaly/2019-07-28/fgoalssipn/2019-12-08_fgoalssipn.nc'\n",
    "cfile='/home/disk/sipn/nicway/data/model/MME_NEW/forecast/sipn_nc/sic/anomaly/2019-07-28/fgoalssipn/2019-10-06_fgoalssipn.nc'\n",
    "ds=xr.open_mfdataset(cfile)\n",
    "print(ds.anomaly.sum(dim=['x','y']).values)\n",
    "#ds.anomaly.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = True   # this is the old way before tried to make it a bit faster without changing answers\n",
    "\n",
    "if not skip:\n",
    "\n",
    "    # Plot all Models\n",
    "    for cmod in models_2_plot:\n",
    "        print(cmod)\n",
    "\n",
    "        # Load in Model\n",
    "        # Find only files that have current year and month in filename (speeds up loading)\n",
    "        all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*.nc') \n",
    "    #    all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*201906*.nc') \n",
    "\n",
    "        # Check we have files \n",
    "        files = glob.glob(all_files)\n",
    "        if not files:\n",
    "            continue # Skip this model\n",
    "\n",
    "        # Get list of variablse we want to drop\n",
    "        drop_vars = [x for x in xr.open_dataset(sorted(files)[-1],autoclose=True).data_vars if x not in variables]\n",
    "        print('The files in range of interest are ',files)\n",
    "        # Load in model   \n",
    "        ds_model_ALL = xr.open_mfdataset(sorted(files), \n",
    "                                     chunks={ 'fore_time': 1,'init_time': 1,'nj': 304, 'ni': 448},  \n",
    "                                     concat_dim='init_time', autoclose=True, \n",
    "                                     parallel=True, drop_variables=drop_vars)\n",
    "                                     # preprocess=lambda x : is_in_time_range(x)) # 'fore_time': 1, ,\n",
    "        ds_model_ALL.rename({'nj':'x', 'ni':'y'}, inplace=True)\n",
    "\n",
    "        # Sort by init_time\n",
    "        ds_model_ALL = ds_model_ALL.sortby('init_time')\n",
    "\n",
    "        # Get Valid time\n",
    "        ds_model_ALL = import_data.get_valid_time(ds_model_ALL)\n",
    "        print('init times are ',ds_model_ALL.init_time.values)\n",
    "\n",
    "\n",
    "        # For each variable\n",
    "        for cvar in variables:\n",
    "\n",
    "            # For each init time period\n",
    "            for it in init_slice: \n",
    "                it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "                # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "                # So we need to add one day, so we don't double count.\n",
    "                print('it_start  ',it_start,\"to\",it)\n",
    "\n",
    "                # For each forecast time we haven't plotted yet\n",
    "                #ft_to_plot = ds_status.sel(init_time=it)\n",
    "                #ft_to_plot = ft_to_plot.where(ft_to_plot.isnull(), drop=True).fore_time\n",
    "\n",
    "                for ft in fore_slice.values: \n",
    "\n",
    "                    cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "                    cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "                    # Get datetime64 of valid time start and end\n",
    "                    valid_start = it_start + ft\n",
    "                    valid_end = it + ft\n",
    "\n",
    "                    # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "                    for metric in metrics_all[cvar]:\n",
    "\n",
    "                        # File paths and stuff\n",
    "                        out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                        if not os.path.exists(out_metric_dir):\n",
    "                            os.makedirs(out_metric_dir) \n",
    "\n",
    "                        out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                        if not os.path.exists(out_init_dir):\n",
    "                            os.makedirs(out_init_dir)\n",
    "\n",
    "                        out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                        if not os.path.exists(out_mod_dir):\n",
    "                            os.makedirs(out_mod_dir)     \n",
    "\n",
    "                        out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "    #                    print((os.path.isfile(out_nc_file)), out_nc_file)\n",
    "\n",
    "                        # Only update if either we are updating All or it doesn't yet exist\n",
    "                        # OR, its one of the last 3 init times \n",
    "                        if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-2:]):\n",
    "                            #print(\"    Updating...\")\n",
    "\n",
    "                            # Select init period and fore_time of interest\n",
    "                            ds_model = ds_model_ALL.sel(init_time=slice(it_start, it))\n",
    "\n",
    "                            # Check we found any init_times in range\n",
    "                            if ds_model.init_time.size==0:\n",
    "                                #print('init_time not found.')\n",
    "                                continue\n",
    "\n",
    "                            # Select var of interest (if available)\n",
    "                            if cvar in ds_model.variables:\n",
    "                                ds_model = ds_model[cvar]\n",
    "                            else:\n",
    "                                #print('cvar not found.')\n",
    "                                continue\n",
    "\n",
    "                            # Check if we have any valid times in range of target dates\n",
    "                            ds_model = ds_model.where((ds_model.valid_time>=valid_start) & (ds_model.valid_time<=valid_end), drop=True) \n",
    "                            if ds_model.fore_time.size == 0:\n",
    "                                #print(\"no fore_time found for target period.\")\n",
    "                                continue\n",
    "                            print('Averaging over fore_time and init_time for variable ds_model: ')\n",
    "                            print(ds_model)\n",
    "                            # Average over for_time and init_times\n",
    "                            ds_model = ds_model.mean(dim=['fore_time','init_time'])\n",
    "\n",
    "                            print('Got the average now computing metric: ',metric)\n",
    "\n",
    "\n",
    "                            if metric=='mean': # Calc ensemble mean\n",
    "                                ds_model = ds_model.mean(dim='ensemble')\n",
    "\n",
    "                            elif metric=='SIP': # Calc probability\n",
    "                                # Remove ensemble members having missing data\n",
    "                                ok_ens = ((ds_model.notnull().sum(dim='x').sum(dim='y'))>0) # select ensemble members with any data\n",
    "                                ds_model = ((ds_model.where(ok_ens, drop=True)>=0.15) ).mean(dim='ensemble').where(ds_model.isel(ensemble=0).notnull())\n",
    "\n",
    "                            elif metric=='anomaly': # Calc anomaly in reference to mean observed 1980-2010\n",
    "                                # Get climatological mean\n",
    "                                da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "                                # Calc anomaly\n",
    "                                ds_model = ds_model.mean(dim='ensemble') - da_obs_mean\n",
    "                                # Add back lat/long (get dropped because of round off differences)\n",
    "                                ds_model['lat'] = da_obs_mean.lat\n",
    "                                ds_model['lon'] = da_obs_mean.lon\n",
    "                            else:\n",
    "                                raise ValueError('metric not implemented')\n",
    "\n",
    "                            print('Done computing metric, our da is now: ', ds_model)\n",
    "\n",
    "                            # drop ensemble if still present\n",
    "                            if 'ensemble' in ds_model:\n",
    "                                ds_model = ds_model.drop('ensemble')\n",
    "\n",
    "                            ds_model.coords['model'] = cmod\n",
    "                            if 'xm' in ds_model:\n",
    "                                ds_model = ds_model.drop(['xm','ym']) #Dump coords we don't use\n",
    "\n",
    "                            # Add Coords info\n",
    "                            ds_model.name = metric\n",
    "                            ds_model.coords['model'] = cmod\n",
    "                            ds_model.coords['init_start'] = it_start\n",
    "                            ds_model.coords['init_end'] = it\n",
    "                            ds_model.coords['valid_start'] = it_start+ft\n",
    "                            ds_model.coords['valid_end'] = it+ft\n",
    "                            ds_model.coords['fore_time'] = ft\n",
    "\n",
    "                            # Save to file\n",
    "                            ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "                            # Clean up for current model\n",
    "                            ds_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#          climatology  trend                             #\n",
    "###########################################################\n",
    "\n",
    "cmod = 'climatology'\n",
    "\n",
    "all_files = os.path.join(mod_dir,cmod,runType,'sipn_nc', '*.nc')\n",
    "files = glob.glob(all_files)\n",
    "\n",
    "obs_clim_model = xr.open_mfdataset(sorted(files), \n",
    "        chunks={'time': 30, 'x': 304, 'y': 448},  \n",
    "         concat_dim='time', autoclose=True, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each variable\n",
    "for cvar in variables:\n",
    "\n",
    "    # For each init time period\n",
    "    for it in init_slice: \n",
    "        it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "        # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "        # So we need to add one day, so we don't double count.\n",
    "        print(it_start,\"to\",it)\n",
    "\n",
    "        for ft in fore_slice.values: \n",
    "\n",
    "            cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "            cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "            # Get datetime64 of valid time start and end\n",
    "            valid_start = it_start + ft\n",
    "            valid_end = it + ft\n",
    "#            print('valid_end ', valid_end)\n",
    "\n",
    "            # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "            for metric in metrics_all[cvar]:\n",
    "\n",
    "                # File paths and stuff\n",
    "                out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                if not os.path.exists(out_metric_dir):\n",
    "                    os.makedirs(out_metric_dir) \n",
    "\n",
    "                out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                if not os.path.exists(out_init_dir):\n",
    "                    os.makedirs(out_init_dir)\n",
    "\n",
    "                out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                if not os.path.exists(out_mod_dir):\n",
    "                    os.makedirs(out_mod_dir)     \n",
    "\n",
    "                out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "                # Only update if either we are updating All or it doesn't yet exist\n",
    "                # OR, its one of the last 3 init times \n",
    "                if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-2:]):\n",
    "                    #print(\"    Updating...\")\n",
    "\n",
    "                    # Check if we have any valid times in range of target dates\n",
    "                    ds_model = obs_clim_model[cvar].where((obs_clim_model.time>=valid_start) & (obs_clim_model.time<=valid_end), drop=True) \n",
    "                    if 'time' in ds_model.lat.dims:\n",
    "                        ds_model.coords['lat'] = ds_model.lat.isel(time=0).drop('time') # Drop time from lat/lon dims (not sure why?)\n",
    "\n",
    "                    # If we have any time\n",
    "                    if ds_model.time.size > 0:\n",
    "\n",
    "                        # Average over time\n",
    "                        ds_model = ds_model.mean(dim='time')\n",
    "\n",
    "                        if metric=='mean': # Calc ensemble mean\n",
    "                            ds_model = ds_model\n",
    "                        elif metric=='SIP': # Calc probability\n",
    "                            # Issue of some ensemble members having missing data\n",
    "                            ocnmask = ds_model.notnull()\n",
    "                            ds_model = (ds_model>=0.15).where(ocnmask)\n",
    "                        elif metric=='anomaly': # Calc anomaly in reference to mean observed 1980-2010\n",
    "                            # Get climatological mean\n",
    "                            da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "                            # Get anomaly\n",
    "                            ds_model = ds_model - da_obs_mean\n",
    "                            # Add back lat/long (get dropped because of round off differences)\n",
    "                            ds_model['lat'] = da_obs_mean.lat\n",
    "                            ds_model['lon'] = da_obs_mean.lon\n",
    "                        else:\n",
    "                            raise ValueError('metric not implemented')   \n",
    "\n",
    "                        # Drop un-needed coords to match model format\n",
    "                        if 'doy' in ds_model.coords:\n",
    "                            ds_model = ds_model.drop(['doy'])\n",
    "                        if 'xm' in ds_model.coords:\n",
    "                            ds_model = ds_model.drop(['xm'])\n",
    "                        if 'ym' in ds_model.coords:\n",
    "                            ds_model = ds_model.drop(['ym'])\n",
    "                    \n",
    "                        # Add Coords info\n",
    "                        ds_model.name = metric\n",
    "                        ds_model.coords['model'] = cmod\n",
    "                        ds_model.coords['init_start'] = it_start\n",
    "                        ds_model.coords['init_end'] = it\n",
    "                        ds_model.coords['valid_start'] = it_start+ft\n",
    "                        ds_model.coords['valid_end'] = it+ft\n",
    "                        ds_model.coords['fore_time'] = ft\n",
    "                        \n",
    "                        # Save to file\n",
    "                        ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "                        # Clean up for current model\n",
    "                        ds_model = None\n",
    "                        \n",
    "obs_clim_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#                               OBSERVATIONS                               #\n",
    "############################################################################\n",
    "\n",
    "cmod = 'Observed'\n",
    "\n",
    "updateAll = True # We ALWAYS want to update all observations for recent past\n",
    "\n",
    "# make init_start_date about 1 mon ago\n",
    "\n",
    "init_start_date = np.datetime64('today') - np.timedelta64(32,'D')\n",
    "\n",
    "print('init_start_date ',init_start_date)\n",
    "    \n",
    "init_slice = np.arange(start_t, cd, datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "init_slice = init_slice[init_slice>=init_start_date] # Select only the inits after init_start_date\n",
    "\n",
    "print(init_slice[0],init_slice[-1])\n",
    "\n",
    "\n",
    "# For each variable\n",
    "for cvar in variables:\n",
    "\n",
    "    # For each init time period\n",
    "    for it in init_slice: \n",
    "        it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "        # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "        # So we need to add one day, so we don't double count.\n",
    "        print(it_start,\"to\",it)\n",
    "\n",
    "        for ft in fore_slice.values: \n",
    "\n",
    "            cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "            cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "            # Get datetime64 of valid time start and end\n",
    "            valid_start = it_start + ft\n",
    "            valid_end = it + ft\n",
    "\n",
    "            # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "            for metric in metrics_all[cvar]:\n",
    "\n",
    "                # File paths and stuff\n",
    "                out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                if not os.path.exists(out_metric_dir):\n",
    "                    os.makedirs(out_metric_dir) \n",
    "\n",
    "                out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                if not os.path.exists(out_init_dir):\n",
    "                    os.makedirs(out_init_dir)\n",
    "\n",
    "                out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                if not os.path.exists(out_mod_dir):\n",
    "                    os.makedirs(out_mod_dir)     \n",
    "\n",
    "                out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "                # Only update if either we are updating All or it doesn't yet exist\n",
    "                # OR, its one of the last 3 init times \n",
    "                if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-2:]):\n",
    "                    #print(\"    Updating...\")\n",
    "\n",
    "                    # Check if we have any valid times in range of target dates\n",
    "                    ds_model = da_obs_c = ds_81[cvar].sel(time=slice(valid_start, valid_end))\n",
    "                    \n",
    "                    if 'time' in ds_model.lat.dims:\n",
    "                        ds_model.coords['lat'] = ds_model.lat.isel(time=0).drop('time') # Drop time from lat/lon dims (not sure why?)\n",
    "\n",
    "                    # Check we have all observations for this week (7)\n",
    "                    if ds_model.time.size == 7:\n",
    "\n",
    "                        if metric=='mean':\n",
    "                            ds_model = ds_model.mean(dim='time') #ds_81.sic.sel(time=(it + ft))\n",
    "                        elif metric=='SIP':\n",
    "                            ds_model = (ds_model >= 0.15).mean(dim='time').astype('int').where(ds_model.isel(time=0).notnull())\n",
    "                        elif metric=='anomaly':\n",
    "                            da_obs_VT = ds_model.mean(dim='time')\n",
    "                            da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "                            ds_model = da_obs_VT - da_obs_mean\n",
    "                        else:\n",
    "                            raise ValueError('Not implemented')\n",
    "\n",
    "                        # Drop coords we don't need\n",
    "                        ds_model = ds_model.drop(['hole_mask','xm','ym'])\n",
    "                        if 'time' in ds_model:\n",
    "                            ds_model = ds_model.drop('time')\n",
    "\n",
    "                        # Add Coords info\n",
    "                        ds_model.name = metric\n",
    "                        ds_model.coords['model'] = cmod\n",
    "                        ds_model.coords['init_start'] = it_start\n",
    "                        ds_model.coords['init_end'] = it\n",
    "                        ds_model.coords['valid_start'] = it_start+ft\n",
    "                        ds_model.coords['valid_end'] = it+ft\n",
    "                        ds_model.coords['fore_time'] = ft\n",
    "\n",
    "                        # Write to disk\n",
    "                        ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "                        # Clean up for current model\n",
    "                        ds_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished Calculating Weekly means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# client = Client(n_workers=8)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvar = 'sic' # hard coded for now\n",
    "# # Load in all data and write to Zarr\n",
    "# # Load in all metrics for given variable\n",
    "# print(\"Loading in weekly metrics...\")\n",
    "# ds_m = import_data.load_MME_by_init_end(E=E, \n",
    "#                                         runType=runType, \n",
    "#                                         variable=cvar, \n",
    "#                                         metrics=metrics_all[cvar])\n",
    "\n",
    "# # Drop models that we don't evaluate (i.e. monthly means)\n",
    "# models_keep = [x for x in ds_m.model.values if x not in ['noaasipn','modcansipns_3','modcansipns_4']]\n",
    "# ds_m = ds_m.sel(model=models_keep)\n",
    "# # Get list of dynamical models that are not observations\n",
    "# dynamical_Models = [x for x in ds_m.model.values if x not in ['Observed','climatology','dampedAnomaly','dampedAnomalyTrend']]\n",
    "# # # Get list of all models\n",
    "# # all_Models = [x for x in ds_m.model.values if x not in ['Observed']]\n",
    "# # Add MME\n",
    "# MME_avg = ds_m.sel(model=dynamical_Models).mean(dim='model') # only take mean over dynamical models\n",
    "# MME_avg.coords['model'] = 'MME'\n",
    "# ds_ALL = xr.concat([ds_m, MME_avg], dim='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ###### ADD METADATA #################\n",
    "\n",
    "# ## Add coordinate system info\n",
    "# ds_ALL.coords['crs'] = xr.DataArray('crs')\n",
    "# ds_ALL['crs'].attrs = {\n",
    "#     'comment': '(https://nsidc.org/data/polar-stereo/ps_grids.html or https://nsidc.org/data/oib/epsg_3413.html) This is a container variable that describes the grid_mapping used by the data in this file. This variable does not contain any data; only information about the geographic coordinate system',\n",
    "#     'grid_mapping_name': 'polar_stereographic',\n",
    "#     'straight_vertical_longitude_from_pole':'-45',\n",
    "#     'latitude_of_projection_origin': '90.0',\n",
    "#     'standard_parallel':'70',\n",
    "#     'false_easting':'0',\n",
    "#     'false_northing':'0'\n",
    "#     }\n",
    "\n",
    "# # Add time coords\n",
    "# ds_ALL.coords['init_start'] = ds_ALL.init_end - np.timedelta64(Ndays,'D') + np.timedelta64(1,'D')\n",
    "# ds_ALL['init_start'].attrs = {\n",
    "#     'comment':        'Start date for weekly average period',\n",
    "#     'long_name':      'Start date for weekly average period',\n",
    "#     'standard_name':  \"start_init_date\"}\n",
    "\n",
    "# ds_ALL['init_end'].attrs = {\n",
    "#     'comment':        'End date for weekly average period',\n",
    "#     'long_name':      'End date for weekly average period',\n",
    "#     'standard_name':  \"end_init_date\"}\n",
    "\n",
    "# ds_ALL['fore_time'].attrs = {\n",
    "#     'comment':        'Forecast lead time',\n",
    "#     'long_name':      'Forecast lead time',\n",
    "#     'standard_name':  \"forecast_lead_time\"}\n",
    "\n",
    "# # Add Valid time (start and end period)\n",
    "# ds_ALL = import_data.get_valid_time(ds_ALL, init_dim='init_end', fore_dim='fore_time')\n",
    "# ds_ALL.rename({'valid_time':'valid_end'}, inplace=True);\n",
    "# ds_ALL.coords['valid_start'] = ds_ALL.valid_end - np.timedelta64(Ndays,'D') + np.timedelta64(1,'D')\n",
    "\n",
    "# # Add attributes\n",
    "# ds_ALL['valid_end'].attrs = {\n",
    "#     'comment':        'End Valid date for weekly average period',\n",
    "#     'long_name':      'End Valid date for weekly average period',\n",
    "#     'standard_name':  \"end_valid_date\"}\n",
    "\n",
    "# ds_ALL['valid_start'].attrs = {\n",
    "#     'comment':        'Start Valid date for weekly average period',\n",
    "#     'long_name':      'Start Valid date for weekly average period',\n",
    "#     'standard_name':  \"start_valid_date\"}\n",
    "\n",
    "# # Add Variable attributes\n",
    "# ds_ALL['SIP'].attrs = {\n",
    "#     'comment':        'Sea ice probability, calculated by averaging across ensemble members predictions of sea ice concentration >= 0.15',\n",
    "#     'grid_mapping':   'crs',\n",
    "#     'long_name':      'Sea ice probability',\n",
    "#     'standard_name':  \"sea_ice_probability\",\n",
    "#     'units':          'fraction'}\n",
    "\n",
    "# ds_ALL['anomaly'].attrs = {\n",
    "#     'comment':        'Anomaly of the forecasted sea ice concentration mean (ensemble average) compared to the 1980 to 2010 Observed Climatology',\n",
    "#     'grid_mapping':   'crs',\n",
    "#     'long_name':      'Anomaly',\n",
    "#     'standard_name':  \"anomaly\",\n",
    "#     'units':          'fraction'}\n",
    "\n",
    "# ds_ALL['mean'].attrs = {\n",
    "#     'comment':        'Mean of the forecasted sea ice concentration (ensemble average)',\n",
    "#     'grid_mapping':   'crs',\n",
    "#     'long_name':      'Sea ice concentration',\n",
    "#     'standard_name':  \"sea_ice_concentration\",\n",
    "#     'units':          'fraction'}\n",
    "\n",
    "# # Dataset Attributes\n",
    "# ds_ALL.attrs = {\n",
    "# 'comment':                         'Weekly mean sea ice concentration forecasted by multiple models as well as observed by remotly sensed passive microwave sensors.',\n",
    "# 'contact':                         'nicway@uw.edu',\n",
    "# 'creator_email':                   'nicway@uw.edu',\n",
    "# 'creator_name':                    'Nicholas Wayand, University of Washington',\n",
    "# 'creator_url':                     'https://atmos.uw.edu/sipn/',\n",
    "# 'date_created':                    '2018-12-03T00:00:00',\n",
    "# 'date_modified':                   datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "# 'geospatial_lat_max':              str(float(ds_ALL.lat.max().values)),\n",
    "# 'geospatial_lat_min':              str(float(ds_ALL.lat.min().values)),\n",
    "# 'geospatial_lat_resolution':       '~25km',\n",
    "# 'geospatial_lat_units':            'degrees_north',\n",
    "# 'geospatial_lon_max':              str(float(ds_ALL.lon.max().values)),\n",
    "# 'geospatial_lon_min':              str(float(ds_ALL.lon.min().values)),\n",
    "# 'geospatial_lon_resolution':       '~25km',\n",
    "# 'geospatial_lon_units':            'degrees_east',\n",
    "# 'history':                         datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')+': updated by Nicholas Wayand',\n",
    "# 'institution':                     'UW, SIPN, ARCUS',\n",
    "# 'keywords':                        'Arctic > Sea ice concentration > Prediction',\n",
    "# 'product_version':                 '1.0',\n",
    "# 'project':                         'Sea Ice Prediction Network Phase II',\n",
    "# 'references':                      'Wayand, N.E., Bitz, C.M., and E. Blanchard-Wrigglesworth, (in review). A year-round sub-seasonal to seasonal sea ice prediction portal. Submited to Geophysical Research letters.',\n",
    "# 'source':                          'Numerical model predictions and Passive microwave measurments.',\n",
    "# 'summary':                         'Dataset is updated daily with weekly sea ice forecasts',\n",
    "# 'time_coverage_end':               pd.to_datetime(ds_ALL.valid_end.max().values).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "# 'time_coverage_start':             pd.to_datetime(ds_ALL.init_start.min().values).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "# 'title':                           'SIPN2 Sea ice Concentration Forecasts and Observations.'\n",
    "# }\n",
    "\n",
    "# ####################################\n",
    "# print(ds_ALL)\n",
    "\n",
    "\n",
    "# # Rechunk from ~1MB to 100MB chunks\n",
    "# # Chunk along fore_time and init_end\n",
    "# ds_ALL = ds_ALL.chunk({'fore_time': 10, 'init_end': 10, 'model': 1, 'x': 304, 'y': 448})\n",
    "\n",
    "# # Save to Zarr\n",
    "# print(\"Saving to Zarr...\")\n",
    "# ds_ALL.to_zarr(os.path.join(E.data_dir,'model/zarr', cvar+'.zarr'), mode='w')\n",
    "# print(\"Finished updating Weekly SIC metrics and saved to Zar\")\n",
    "# ds_ALL=None # Flush memory\n",
    "# MME_avg=None\n",
    "# ds_m=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
