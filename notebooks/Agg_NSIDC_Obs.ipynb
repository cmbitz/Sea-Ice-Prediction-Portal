{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "'''\n",
    "For the 3 SIC obs datasets. Accumulates the daily files into yearly, saves files\n",
    "Then resamples each year into weeks and saves by year\n",
    "Also compute regional extents\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt, mpld3\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ESIO Imports\n",
    "\n",
    "from esio import EsioData as ed\n",
    "from esio import metrics\n",
    "\n",
    "import dask\n",
    "# from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c = Client()\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this day is  2019-05-08 09:18:46.792482  current year 2019\n"
     ]
    }
   ],
   "source": [
    "cd = datetime.datetime.now()\n",
    "cy = cd.year   # current year\n",
    "#cd = datetime.datetime(cy, 1, 2)  # force it for checking\n",
    "\n",
    "print('this day is ',cd, ' current year', cy)\n",
    "firstfive = False\n",
    "if ((cd.month == 1) & (cd.day<6)):\n",
    "    firstfive = True\n",
    "firstfive\n",
    "\n",
    "E = ed.EsioData.load()\n",
    "data_dir = E.obs_dir\n",
    "mod_dir = E.model_dir\n",
    "\n",
    "# Products to import\n",
    "product_list = ['NSIDC_0081', 'NSIDC_0079'] # , 'NSIDC_0051']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating  NSIDC_0081 ...\n",
      "Year  2015  is done\n",
      "Year  2016  is done\n",
      "Year  2017  is done\n",
      "Not first few days of year so do not redo 2018\n",
      "writing netcdf file\n",
      "2019\n",
      "Finished  NSIDC_0081\n",
      "\n",
      "Aggregating  NSIDC_0079 ...\n",
      "Year  1979  is done\n",
      "Year  1980  is done\n",
      "Year  1981  is done\n",
      "Year  1982  is done\n",
      "Year  1983  is done\n",
      "Year  1984  is done\n",
      "Year  1985  is done\n",
      "Year  1986  is done\n",
      "Year  1987  is done\n",
      "Year  1988  is done\n",
      "Year  1989  is done\n",
      "Year  1990  is done\n",
      "Year  1991  is done\n",
      "Year  1992  is done\n",
      "Year  1993  is done\n",
      "Year  1994  is done\n",
      "Year  1995  is done\n",
      "Year  1996  is done\n",
      "Year  1997  is done\n",
      "Year  1998  is done\n",
      "Year  1999  is done\n",
      "Year  2000  is done\n",
      "Year  2001  is done\n",
      "Year  2002  is done\n",
      "Year  2003  is done\n",
      "Year  2004  is done\n",
      "Year  2005  is done\n",
      "Year  2006  is done\n",
      "Year  2007  is done\n",
      "Year  2008  is done\n",
      "Year  2009  is done\n",
      "Year  2010  is done\n",
      "Year  2011  is done\n",
      "Year  2012  is done\n",
      "Year  2013  is done\n",
      "Year  2014  is done\n",
      "Year  2015  is done\n",
      "Year  2016  is done\n",
      "Year  2017  is done\n",
      "Not first few days of year so do not redo 2018\n",
      "Finished  NSIDC_0079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gather all days from a given years into one file\n",
    "\n",
    "# Loop through each product\n",
    "for c_product in product_list:\n",
    "    print('Aggregating ', c_product, '...')\n",
    "\n",
    "    for cyear in np.arange(1979,cy+1,1):\n",
    "        #print(cyear)\n",
    "        \n",
    "        cyear_str = str(cyear)\n",
    "        \n",
    "        out_dir = os.path.join(data_dir, c_product, 'sipn_nc_yearly')\n",
    "        if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "                \n",
    "        nc_out = os.path.join(out_dir, cyear_str+'.nc')\n",
    "        # Don't update file if exits, unless current year or in first 5 days of new year\n",
    "        if ((os.path.isfile(nc_out)) & (cyear<cy-1)):\n",
    "            print('Year ',cyear,' is done')\n",
    "            continue\n",
    "        if ((os.path.isfile(nc_out)) & ((cyear==cy-1) & (not(firstfive)))):\n",
    "            print('Not first few days of year so do not redo',cyear)\n",
    "            continue\n",
    "\n",
    "        # Load in Obs\n",
    "        c_files = sorted(glob.glob(E.obs[c_product]['sipn_nc']+'/*_'+cyear_str+'*.nc'))\n",
    "        if len(c_files)==0:\n",
    "            #print(\"No files found for current year\")\n",
    "            continue\n",
    "        ds_year = xr.open_mfdataset(c_files, \n",
    "                                      concat_dim='time', parallel=True)\n",
    "\n",
    "        print('writing netcdf file')\n",
    "        ds_year.to_netcdf(nc_out)\n",
    "        print(cyear)\n",
    "      \n",
    "    # For each Product\n",
    "    print(\"Finished \", c_product)\n",
    "    print(\"\")\n",
    "    \n",
    "ds_year = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample in weeks but always make weeks relative to Jan 1 of year\n",
    "\n",
    "DOW   = ['sun', \n",
    "          'mon', \n",
    "          'tue', \n",
    "          'wed', \n",
    "          'thu',  \n",
    "          'fri', \n",
    "          'sat']\n",
    "\n",
    "# this routine won't work for the years before satelite data was daily\n",
    "\n",
    "# Loop through each product\n",
    "for c_product in product_list:\n",
    "    print('Aggregating into weekly means starting new each year', c_product, '...')\n",
    "\n",
    "    for cyear in np.arange(1989,cy+1,1):\n",
    "        #print(cyear)\n",
    "        \n",
    "        cyear_str = str(cyear)\n",
    "        \n",
    "        data_dir = os.path.join(E.obs_dir, c_product, 'sipn_nc_yearly') # read in daily in year lumps\n",
    "        data_dir_byweek = os.path.join(E.obs_dir, c_product, 'sipn_nc_yearly_byweek') # output weekly in year lumps\n",
    "        nc_daily = os.path.join(data_dir, cyear_str+'.nc')\n",
    "        nc_weeks = os.path.join(data_dir_byweek, cyear_str+'_byweek.nc')\n",
    "        # Don't update file if exits, unless current year or in first 5 days of new year\n",
    "        if (not(os.path.isfile(nc_daily))):\n",
    "#            print(nc_daily,' does not exist')\n",
    "            continue\n",
    "        if ((os.path.isfile(nc_weeks)) & (cyear<cy-1)):\n",
    "            print('Year ',cyear,' is done')\n",
    "            continue\n",
    "        if ((os.path.isfile(nc_weeks)) & ((cyear==cy-1) & (not(firstfive)))):\n",
    "            print('Not first few days of year so do not redo',cyear)\n",
    "            continue\n",
    "\n",
    "        ds_daily = xr.open_mfdataset(nc_daily,data_vars=['sic','extent','area','week'],parallel=True)\n",
    "        ds_daily = ds_daily.drop('hole_mask')\n",
    "#        print(ds_daily)\n",
    "        \n",
    "        print('first DOY', ds_daily.time.isel(time=0).values)\n",
    "        firstDOWnumber=ds_daily['time.dayofweek'].isel(time=0).values\n",
    "        print('first day of year', firstDOWnumber)\n",
    "        wstr='w-'+DOW[firstDOWnumber]\n",
    "        print(wstr)\n",
    "\n",
    "        ds_weekly = ds_daily.resample(time=wstr).mean()\n",
    "        ds_weekly = ds_weekly.isel(time=slice(0,52))\n",
    "        # Add DOY\n",
    "        weeks = np.arange(1,len(ds_weekly.time)+1,1)\n",
    "        ds_weekly.coords['week'] = xr.DataArray(weeks, dims='time', coords={'time':ds_weekly.time})\n",
    "\n",
    "        print('SHOULD be 7th DOY ', ds_weekly.time.isel(time=0).values)\n",
    "#        print('weekly extent ', ds_weekly.extent.values)\n",
    "        print('writing netcdf file for year ',cyear)\n",
    "        ds_weekly.to_netcdf(nc_weeks)\n",
    "      \n",
    "    # For each Product\n",
    "    print(\"Finished \", c_product)\n",
    "    print(\"\")\n",
    "    \n",
    "ds_weekly = None\n",
    "ds_dail = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating into regional extents NSIDC_0081 ...\n",
      "Year  2015  is done\n",
      "Year  2016  is done\n",
      "Year  2017  is done\n",
      "Not first few days of year so do not redo 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/core.py:137: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return func(*args2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing year with missing data\n",
      "Saved  /home/disk/sipn/nicway/data/obs/NSIDC_0081/sipn_nc_yearly_agg/2019.nc\n",
      "Finished  NSIDC_0081\n",
      "\n",
      "Aggregating into regional extents NSIDC_0079 ...\n",
      "Year  1989  is done\n",
      "Year  1990  is done\n",
      "Year  1991  is done\n",
      "Year  1992  is done\n",
      "Year  1993  is done\n",
      "Year  1994  is done\n",
      "Year  1995  is done\n",
      "Year  1996  is done\n",
      "Year  1997  is done\n",
      "Year  1998  is done\n",
      "Year  1999  is done\n",
      "Year  2000  is done\n",
      "Year  2001  is done\n",
      "Year  2002  is done\n",
      "Year  2003  is done\n",
      "Year  2004  is done\n",
      "Year  2005  is done\n",
      "Year  2006  is done\n",
      "Year  2007  is done\n",
      "Year  2008  is done\n",
      "Year  2009  is done\n",
      "Year  2010  is done\n",
      "Year  2011  is done\n",
      "Year  2012  is done\n",
      "Year  2013  is done\n",
      "Year  2014  is done\n",
      "Year  2015  is done\n",
      "Year  2016  is done\n",
      "Year  2017  is done\n",
      "Not first few days of year so do not redo 2018\n",
      "Finished  NSIDC_0079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute regional means starting in 1989\n",
    "\n",
    "# Load in regional data\n",
    "# Note minor -0.000004 degree differences in latitude\n",
    "ds_region = xr.open_dataset(os.path.join(E.grid_dir, 'sio_2016_mask_Update.nc'))\n",
    "\n",
    "\n",
    "# Loop through each product\n",
    "for c_product in product_list:\n",
    "#for c_product in ['NSIDC_0079']:\n",
    "\n",
    "    print('Aggregating into regional extents', c_product, '...')\n",
    "    data_dir = os.path.join(E.obs_dir, c_product, 'sipn_nc_yearly') # read in daily in year lumps\n",
    "    data_dir_agg = os.path.join(E.obs_dir, c_product, 'sipn_nc_yearly_agg') # output weekly in year lumps\n",
    "\n",
    "    for cyear in np.arange(1989,cy+1,1):\n",
    "#    for cyear in [2005]:\n",
    "\n",
    "        #print(cyear)\n",
    "        \n",
    "        cyear_str = str(cyear)\n",
    "        \n",
    "        nc_fullfield = os.path.join(data_dir, cyear_str+'.nc')\n",
    "        nc_agg = os.path.join(data_dir_agg, cyear_str+'.nc')\n",
    "        # Don't update file if exits, unless current year or in first 5 days of new year\n",
    "        if (not(os.path.isfile(nc_fullfield))):\n",
    "#            print(nc_daily,' does not exist')\n",
    "            continue\n",
    "        if ((os.path.isfile(nc_agg)) & (cyear<cy-1)):\n",
    "            print('Year ',cyear,' is done')\n",
    "            continue\n",
    "        if ((os.path.isfile(nc_agg)) & ((cyear==cy-1) & (not(firstfive)))):\n",
    "            print('Not first few days of year so do not redo',cyear)\n",
    "            continue\n",
    "\n",
    "        ds_field = xr.open_mfdataset(nc_fullfield,data_vars=['sic','extent','area','week'],parallel=True)\n",
    "        ds_field = ds_field.drop('hole_mask')\n",
    "#        print(ds_field)\n",
    "\n",
    "        # Calc panArctic extent\n",
    "#        da_panE = metrics.calc_extent(da=ds_field.sic, region=ds_region)\n",
    "        da_panE = ds_field.extent  # verified is the same\n",
    "        da_panE['nregions'] = 99\n",
    "        da_panE['region_names'] = 'panArctic'\n",
    "\n",
    "#        f = plt.figure(figsize=(15,10))\n",
    "#        ax1 = plt.subplot(1, 1, 1) # Observations\n",
    "#        da_panE.plot(ax=ax1, label=str(cyear)+' Observed', color='m', linewidth=8)\n",
    "#        diehere\n",
    "\n",
    "        # Calc Regional extents\n",
    "        da_RegE = metrics.agg_by_domain(da_grid=ds_field.sic, ds_region=ds_region)\n",
    "\n",
    "        # Merge\n",
    "        ds_out = xr.concat([da_panE, da_RegE], dim='nregions')\n",
    "        ds_out.name = 'Extent'\n",
    "#        print(ds_out)\n",
    "\n",
    "        ds_out.load() # This prevents many errors in the dask graph (I don't know why)\n",
    "\n",
    "        if (len(ds_out.time)<365):  # linear interpolate missing day in 2005\n",
    "            print('fixing year with missing data')\n",
    "            ds_out = ds_out.resample(time='1D').interpolate('linear')\n",
    "\n",
    "        # # Save regridded to netcdf file\n",
    "        ds_out.to_netcdf(nc_agg)\n",
    "        ds_out = None # Memory clean up\n",
    "        da_panE = None\n",
    "        da_RegE = None\n",
    "        print('Saved ', nc_agg)\n",
    "\n",
    "    # For each Product\n",
    "    print(\"Finished \", c_product)\n",
    "    print(\"\")\n",
    "    \n",
    "ds_field = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:       (nregions: 15, time: 10719)\n",
      "Coordinates:\n",
      "  * time          (time) datetime64[ns] 1990-01-01 1990-01-02 ... 2019-05-07\n",
      "  * nregions      (nregions) int64 99 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
      "    region_names  (nregions) object 'panArctic' ... 'Central Arctic'\n",
      "    year          (time) int64 1990 1990 1990 1990 1990 ... 2019 2019 2019 2019\n",
      "    doy           (time) int64 1 2 3 4 5 6 7 8 ... 121 122 123 124 125 126 127\n",
      "Data variables:\n",
      "    Extent        (time, nregions) float64 dask.array<shape=(10719, 15), chunksize=(10719, 15)>\n"
     ]
    }
   ],
   "source": [
    "# gather up the regionally aggregated obs for processing, ds_all is smoothed with a 10 day running mean\n",
    "# crud meant it to be 30\n",
    "ds_79 = None\n",
    "ds_81 = None\n",
    "\n",
    "start_year = 1990\n",
    "pred_year = cy + 1\n",
    "#pred_year = 2018   # done already\n",
    "#pred_year = 2019   # done already\n",
    "\n",
    "c_product = 'NSIDC_0081'\n",
    "data_dir_agg = os.path.join(E.obs_dir, c_product, 'sipn_nc_yearly_agg') # output weekly in year lumps\n",
    "nc_agg = os.path.join(data_dir_agg, '*.nc')\n",
    "ds_81 = xr.open_mfdataset(nc_agg)\n",
    "\n",
    "c_product = 'NSIDC_0079'\n",
    "data_dir_agg = os.path.join(E.obs_dir, c_product, 'sipn_nc_yearly_agg') # output weekly in year lumps\n",
    "nc_agg = os.path.join(data_dir_agg, '*.nc')\n",
    "ds_79 = xr.open_mfdataset(nc_agg)\n",
    "\n",
    "ds_79=ds_79.sel(time=slice(str(start_year),str(pred_year-1)))  # end year just has to be way in the future\n",
    "ds_81=ds_81.sel(time=slice('2015',str(pred_year-1)))  # restrict to before prediciton year, lower year not important\n",
    "#print(ds_81)\n",
    "#print(ds_79)\n",
    "\n",
    "ds_all = ds_79.combine_first(ds_81)  # takes ds_79 as priority\n",
    "ds_all = ds_all.sel(time=slice('1990',str(pred_year-1)))\n",
    "ds_81 = None\n",
    "\n",
    "# add year to observations dataset\n",
    "year_all = [x.year for x in pd.to_datetime(ds_all.time.values)]\n",
    "ds_all.coords['year'] = xr.DataArray(year_all, dims='time', coords={'time':ds_all.time})\n",
    "\n",
    "# add doy to observational dataset\n",
    "DOY = [x.timetuple().tm_yday for x in pd.to_datetime(ds_all.time.values)]\n",
    "ds_all.coords['doy'] = xr.DataArray(DOY, dims='time', coords={'time':ds_all.time})\n",
    "ds_rough = ds_all\n",
    "ds_all = ds_all.rolling(time=10, min_periods=1, center=True).mean()\n",
    "print(ds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ds_rough.sel(nregions=99,time=slice('1990','1991')).Extent.plot(color='k')\n",
    "ds_all.sel(nregions=99,time=slice('1990','1991')).Extent.plot(color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestPlot = False\n",
    "if TestPlot:\n",
    "    # Select a day of the year to test\n",
    "    ytrain = ds_all.Extent.sel(nregions=99)\n",
    "    rough = ds_rough.Extent.sel(nregions=99)\n",
    "\n",
    "    ytrain = ytrain.where(ds_all['doy']==104, drop=True).values\n",
    "    rough = rough.where(ds_all['doy']==104, drop=True).values\n",
    "\n",
    "    cyears=np.arange(1990,pred_year,1)\n",
    "\n",
    "    pfit = metrics._lowessfit(cyears, ytrain)  # new method local for mucking\n",
    "#    pfit = _fitparams(cyears, ytrain)  # new method local for mucking\n",
    "    fitfun = np.poly1d(pfit)\n",
    "    newpred = fitfun(pred_year)\n",
    "    \n",
    "    # can I reconstruct it by hand (yes)\n",
    "    tmp=cyears**2*pfit[0]+cyears*pfit[1]+pfit[2]\n",
    "    \n",
    "    f = plt.figure()\n",
    "    plt.plot(cyears,ytrain,marker='o',markersize=10,color='k')\n",
    "    plt.plot(cyears,rough,marker='o',markersize=10,color='g')\n",
    "    plt.plot(pred_year,newpred,marker='o',markersize=10,color='b')\n",
    "    plt.plot(cyears,tmp,marker='o',markersize=10,color='c')\n",
    "\n",
    "    print('cyan dots are quadratic fit to lowess smoothed data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the fit parameters for the regionally aggregated obs\n",
    "# must update each day for current pred_year\n",
    "# pred_year = 2018 already did 2018, 2019 \n",
    "# takes a few minutes done for 2018, 2019\n",
    "c_product = 'NSIDC_0079'  # save here so all fit params are together\n",
    "file_out = os.path.join(E.obs_dir, c_product, 'fitparams','fitparams_1990-'+str(pred_year-1)+'.nc') # read in daily in year lumps\n",
    "\n",
    "print('ds_all ',ds_all)\n",
    "da_out = None\n",
    "\n",
    "# these are the days we can predict \n",
    "doypred = ds_all.where(ds_all['year']==pred_year-1,drop=True).doy.values\n",
    "\n",
    "for cdoy in doypred:\n",
    "    if (cdoy<366):  # do not fit if cdoy is 366 assuming data are lacking\n",
    "        # Select cdoy \n",
    "        thisday=ds_all.Extent.where(ds_all['doy']==cdoy, drop=True).swap_dims({'time':'year'})\n",
    "    #    print('thisday ',thisday)\n",
    "        da = metrics.LowessQuadFit(thisday.chunk({'year': -1}), 'year') # Have to rechunk year into one big one    print(tmp)\n",
    "        da.name = 'fitparams'\n",
    "        da.load()  # load before saving forces calculation now\n",
    "\n",
    "        # Move back to actual (valid_time) space\n",
    "        da = da.expand_dims('time')\n",
    "        da.coords['time'] = xr.DataArray([datetime.datetime(pred_year,1,1) + datetime.timedelta(days=int(x-1)) for x in [cdoy]], dims='time')\n",
    "    #    print(da)\n",
    "        if (cdoy==1):\n",
    "            da_out=da\n",
    "        else:\n",
    "            # Merge\n",
    "            da_out = xr.concat([da_out, da], dim='time')\n",
    "\n",
    "# if on day 365 or 366 be sure to have a day 366 to deal with potention for pred_year needing a leap day\n",
    "if (cdoy>=365):\n",
    "    # repreat day 365 for leap days assuming inssufficient years to do better\n",
    "    da.coords['time'] = xr.DataArray([datetime.datetime(pred_year,1,1) + datetime.timedelta(days=int(x-1)) for x in [366]], dims='time')\n",
    "    da_out = xr.concat([da_out, da], dim='time')\n",
    "\n",
    "# Save to disk\n",
    "da_out.to_netcdf(file_out)\n",
    "print(\"Saved\",file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "<xarray.DataArray 'ClimoTrendExtent' (time: 127, nregions: 15)>\n",
      "dask.array<shape=(127, 15), dtype=float64, chunksize=(127, 15)>\n",
      "Coordinates:\n",
      "  * nregions      (nregions) int64 99 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
      "    region_names  (nregions) object dask.array<shape=(15,), chunksize=(15,)>\n",
      "  * time          (time) datetime64[ns] 2020-01-01 2020-01-02 ... 2020-05-06\n",
      "Saved /home/disk/sipn/nicway/data/obs/NSIDC_0079/sipn_nc_yearly_agg_climatology/2020_RegionalExtents.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/compatibility.py:93: FutureWarning: The autoclose argument is no longer used by xarray.open_dataset() and is now ignored; it will be removed in xarray v0.12. If necessary, you can control the maximum number of simultaneous open files with xarray.set_options(file_cache_maxsize=...).\n",
      "  return func(*args, **kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/xarray/conventions.py:176: SerializationWarning: variable region_names has data in the form of a dask array with dtype=object, which means it is being loaded into memory to determine a data type that can be safely stored on disk. To avoid this, coerce this variable to a fixed-size dtype with astype() before saving it.\n",
      "  SerializationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Compute and Write the climo Trend for each day of the prediction year\n",
    "# must update each day for current pred_year\n",
    "\n",
    "# pred_year = 2018 already did 2018, 2019 \n",
    "\n",
    "c_product = 'NSIDC_0079'  # fit params are here which is kind of dumb\n",
    "file_in = os.path.join(E.obs_dir, c_product, 'fitparams','fitparams_1990-'+str(pred_year-1)+'.nc') # read in daily in year lumps\n",
    "\n",
    "ds = xr.open_mfdataset(file_in, autoclose=True, parallel=True)\n",
    "\n",
    "recons=pred_year**2*ds.fitparams.isel(pdim=0)  +  pred_year*ds.fitparams.isel(pdim=1) +  ds.fitparams.isel(pdim=2)\n",
    "recons.name = 'ClimoTrendExtent'\n",
    "recons = recons.drop('pdim')\n",
    "\n",
    "leapyear = (pred_year//4 )*1.0 == pred_year/4\n",
    "if not leapyear: # & len(recons.time.values)==366:\n",
    "    recons = recons.isel(time=slice(0,365))\n",
    "    print('dropping extra day for non leapyear')\n",
    "\n",
    "file_out = os.path.join(E.obs_dir, c_product, 'sipn_nc_yearly_agg_climatology',str(pred_year)+'_RegionalExtents.nc')\n",
    "\n",
    "recons.to_netcdf(file_out)\n",
    "print(\"Saved\",file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative way to resample into weeks not pursued\n",
    "alternativeway = False\n",
    "\n",
    "if alternativeway:\n",
    "    # another brute force method but the dataarray that is computed is lacking dims and coords\n",
    "    # might not be robust either\n",
    "    DOY = [x.timetuple().tm_yday for x in pd.to_datetime(ds_daily.time.values)]\n",
    "    weeks= (np.ceil(np.divide(DOY,7)))\n",
    "    #print(DOY)\n",
    "    ds_daily.coords['week'] = xr.DataArray(weeks, dims='time', coords={'time':ds_daily.time})\n",
    "    #print(ds_year)\n",
    "    mean_ext = ds_daily.extent.groupby('week').mean(dim='time')\n",
    "\n",
    "    print('mean extent ',mean_ext)\n",
    "    print(mean_ext.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute anomalies of the regional anomalies for purpose of computing alpha for damped persistence\n",
    "# not important to redo with more data since 28 years should give a good estimate for alpha\n",
    "\n",
    "update = False\n",
    "\n",
    "if update:\n",
    "    start_year=1990\n",
    "    end_year = 2017  \n",
    "\n",
    "    file_in = os.path.join(E.obs_dir, c_product, 'fitparams','fitparams_1990-'+str(end_year)+'.nc') # read in daily in year lumps\n",
    "    ds = xr.open_mfdataset(file_in, autoclose=True, parallel=True)\n",
    "\n",
    "    DOY = [x.timetuple().tm_yday for x in pd.to_datetime(ds.time.values)]\n",
    "    ds.coords['doy'] = xr.DataArray(DOY, dims='time', coords={'time':ds.time})\n",
    "    ds['doy'][-1] = 366\n",
    "    ds = ds.swap_dims({'time':'doy'})\n",
    "    ds = ds.drop('time')\n",
    "    print(ds)\n",
    "    \n",
    "    ds_79=ds_all.sel(time=slice(str(start_year),str(end_year)))   # this is already smoothed\n",
    "\n",
    "    for cyear in np.arange(start_year, end_year+1, 1):\n",
    "        file_out = os.path.join(E.obs_dir, c_product,'sipn_nc_yearly_agg_anom', 'RegionalExtentsAnomalies_'+str(cyear)+'.nc')\n",
    "\n",
    "        # current year \n",
    "        ds_specific = ds_79.where(ds_79.year==cyear, drop=True) #.swap_dims({'time':'year'})\n",
    "        \n",
    "#        print('ds_specific ', ds_specific)\n",
    "        cdoys = ds_specific.doy.values\n",
    "#        print('ds.fitparams ',ds.fitparams)\n",
    "\n",
    "        recons=cyear**2*ds.fitparams.sel(pdim=0,doy=cdoys) + cyear*ds.fitparams.sel(pdim=1,doy=cdoys) + ds.fitparams.sel(pdim=2,doy=cdoys)\n",
    "        #x[0]**n * p[0] + ... + x[0] * p[n-1] + p[n] = y[0]\n",
    "\n",
    "        recons.coords['time'] = xr.DataArray(ds_specific['time'].values, dims='doy', coords={'doy':recons.doy})\n",
    "        recons = recons.swap_dims({'doy':'time'})\n",
    "        recons = recons.drop('pdim')\n",
    "        recons = ds_specific - recons  # compute anomaly \n",
    "#        recons = recons.drop('year')\n",
    "#        print('recons after mucking ',recons)\n",
    "\n",
    "#        f = plt.figure(figsize=(15,10))\n",
    "#        ax1 = plt.subplot(1, 1, 1) # Observations\n",
    "#        recons.Extent.sel(nregions=9).plot(ax=ax1, label=str(cyear), color='m', linewidth=8)\n",
    "\n",
    "        recons.to_netcdf(file_out)\n",
    "        print(\"Saved\",file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add a new dimension to dataarray with size of the list fill_value\n",
    "from collections import OrderedDict\n",
    "def expand_dimensions(data, fill_value=np.nan, **new_coords):\n",
    "    ordered_coord_dict = OrderedDict(new_coords)\n",
    "    shape_da = xr.DataArray(\n",
    "        np.zeros(list(map(len, ordered_coord_dict.values()))),\n",
    "        coords=ordered_coord_dict,\n",
    "        dims=ordered_coord_dict.keys())\n",
    "    expanded_data = xr.broadcast(data, shape_da)[0].fillna(fill_value)\n",
    "    return expanded_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allanoms  <xarray.DataArray (year: 10220, nregions: 15)>\n",
      "dask.array<shape=(10220, 15), dtype=float64, chunksize=(10220, 15)>\n",
      "Coordinates:\n",
      "  * nregions      (nregions) int64 99 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
      "    region_names  (nregions) object dask.array<shape=(15,), chunksize=(15,)>\n",
      "  * year          (year) int64 1990 1990 1990 1990 1990 ... 2017 2017 2017 2017\n",
      "    doy           (year) int64 dask.array<shape=(10220,), chunksize=(10220,)>\n",
      "0 1.4372287103107464\n",
      "1 0.44854106970544905\n",
      "2 0.5407421150457135\n",
      "3 0.5665224193931717\n",
      "4 0.1598909382307524\n",
      "5 0.41945648840451866\n",
      "6 0.4132245981080027\n",
      "7 0.40627174741700445\n",
      "8 0.4113040611640081\n",
      "9 0.5646755833965562\n",
      "10 0.49232154652934\n",
      "11 0.2675288373989181\n",
      "12 0.23389659183960743\n",
      "13 0.12732286965148276\n",
      "Processing day of year  1\n",
      "Processing day of year  2\n",
      "Processing day of year  3\n",
      "Processing day of year  4\n",
      "Processing day of year  5\n",
      "Processing day of year  6\n",
      "Processing day of year  7\n",
      "Processing day of year  8\n",
      "Processing day of year  9\n",
      "Processing day of year  10\n",
      "Processing day of year  11\n",
      "Processing day of year  12\n",
      "Processing day of year  13\n",
      "Processing day of year  14\n",
      "Processing day of year  15\n",
      "Processing day of year  16\n",
      "Processing day of year  17\n",
      "Processing day of year  18\n",
      "Processing day of year  19\n",
      "Processing day of year  20\n",
      "Processing day of year  21\n",
      "Processing day of year  22\n",
      "Processing day of year  23\n",
      "Processing day of year  24\n",
      "Processing day of year  25\n",
      "Processing day of year  26\n",
      "Processing day of year  27\n",
      "Processing day of year  28\n",
      "Processing day of year  29\n",
      "Processing day of year  30\n",
      "Processing day of year  31\n",
      "Processing day of year  32\n",
      "Processing day of year  33\n",
      "Processing day of year  34\n",
      "Processing day of year  35\n",
      "Processing day of year  36\n",
      "Processing day of year  37\n",
      "Processing day of year  38\n",
      "Processing day of year  39\n",
      "Processing day of year  40\n",
      "Processing day of year  41\n",
      "Processing day of year  42\n",
      "Processing day of year  43\n",
      "Processing day of year  44\n",
      "Processing day of year  45\n",
      "Processing day of year  46\n",
      "Processing day of year  47\n",
      "Processing day of year  48\n",
      "Processing day of year  49\n",
      "Processing day of year  50\n",
      "Processing day of year  51\n",
      "Processing day of year  52\n",
      "Processing day of year  53\n",
      "Processing day of year  54\n",
      "Processing day of year  55\n",
      "Processing day of year  56\n",
      "Processing day of year  57\n",
      "Processing day of year  58\n",
      "Processing day of year  59\n",
      "Processing day of year  60\n",
      "Processing day of year  61\n",
      "Processing day of year  62\n",
      "Processing day of year  63\n",
      "Processing day of year  64\n",
      "Processing day of year  65\n",
      "Processing day of year  66\n",
      "Processing day of year  67\n",
      "Processing day of year  68\n",
      "Processing day of year  69\n",
      "Processing day of year  70\n",
      "Processing day of year  71\n",
      "Processing day of year  72\n",
      "Processing day of year  73\n",
      "Processing day of year  74\n",
      "Processing day of year  75\n",
      "Processing day of year  76\n",
      "Processing day of year  77\n",
      "Processing day of year  78\n",
      "Processing day of year  79\n",
      "Processing day of year  80\n",
      "Processing day of year  81\n",
      "Processing day of year  82\n",
      "Processing day of year  83\n",
      "Processing day of year  84\n",
      "Processing day of year  85\n",
      "Processing day of year  86\n",
      "Processing day of year  87\n",
      "Processing day of year  88\n",
      "Processing day of year  89\n",
      "Processing day of year  90\n",
      "Processing day of year  91\n",
      "Processing day of year  92\n",
      "Processing day of year  93\n",
      "Processing day of year  94\n",
      "Processing day of year  95\n",
      "Processing day of year  96\n",
      "Processing day of year  97\n",
      "Processing day of year  98\n",
      "Processing day of year  99\n",
      "Processing day of year  100\n",
      "Processing day of year  101\n",
      "Processing day of year  102\n",
      "Processing day of year  103\n",
      "Processing day of year  104\n",
      "Processing day of year  105\n",
      "Processing day of year  106\n",
      "Processing day of year  107\n",
      "Processing day of year  108\n",
      "Processing day of year  109\n",
      "Processing day of year  110\n",
      "Processing day of year  111\n",
      "Processing day of year  112\n",
      "Processing day of year  113\n",
      "Processing day of year  114\n",
      "Processing day of year  115\n",
      "Processing day of year  116\n",
      "Processing day of year  117\n",
      "Processing day of year  118\n",
      "Processing day of year  119\n",
      "Processing day of year  120\n",
      "Processing day of year  121\n",
      "Processing day of year  122\n",
      "Processing day of year  123\n",
      "Processing day of year  124\n",
      "Processing day of year  125\n",
      "Processing day of year  126\n",
      "Processing day of year  127\n",
      "Processing day of year  128\n",
      "Processing day of year  129\n",
      "Processing day of year  130\n",
      "Processing day of year  131\n",
      "Processing day of year  132\n",
      "Processing day of year  133\n",
      "Processing day of year  134\n",
      "Processing day of year  135\n",
      "Processing day of year  136\n",
      "Processing day of year  137\n",
      "Processing day of year  138\n",
      "Processing day of year  139\n",
      "Processing day of year  140\n",
      "Processing day of year  141\n",
      "Processing day of year  142\n",
      "Processing day of year  143\n",
      "Processing day of year  144\n",
      "Processing day of year  145\n",
      "Processing day of year  146\n",
      "Processing day of year  147\n",
      "Processing day of year  148\n",
      "Processing day of year  149\n",
      "Processing day of year  150\n",
      "Processing day of year  151\n",
      "Processing day of year  152\n",
      "Processing day of year  153\n",
      "Processing day of year  154\n",
      "Processing day of year  155\n",
      "Processing day of year  156\n",
      "Processing day of year  157\n",
      "Processing day of year  158\n",
      "Processing day of year  159\n",
      "Processing day of year  160\n",
      "Processing day of year  161\n",
      "Processing day of year  162\n",
      "Processing day of year  163\n",
      "Processing day of year  164\n",
      "Processing day of year  165\n",
      "Processing day of year  166\n",
      "Processing day of year  167\n",
      "Processing day of year  168\n",
      "Processing day of year  169\n",
      "Processing day of year  170\n",
      "Processing day of year  171\n",
      "Processing day of year  172\n",
      "Processing day of year  173\n",
      "Processing day of year  174\n",
      "Processing day of year  175\n",
      "Processing day of year  176\n",
      "Processing day of year  177\n",
      "Processing day of year  178\n",
      "Processing day of year  179\n",
      "Processing day of year  180\n",
      "Processing day of year  181\n",
      "Processing day of year  182\n",
      "Processing day of year  183\n",
      "Processing day of year  184\n",
      "Processing day of year  185\n",
      "Processing day of year  186\n",
      "Processing day of year  187\n",
      "Processing day of year  188\n",
      "Processing day of year  189\n",
      "Processing day of year  190\n",
      "Processing day of year  191\n",
      "Processing day of year  192\n",
      "Processing day of year  193\n",
      "Processing day of year  194\n",
      "Processing day of year  195\n",
      "Processing day of year  196\n",
      "Processing day of year  197\n",
      "Processing day of year  198\n",
      "Processing day of year  199\n",
      "Processing day of year  200\n",
      "Processing day of year  201\n",
      "Processing day of year  202\n",
      "Processing day of year  203\n",
      "Processing day of year  204\n",
      "Processing day of year  205\n",
      "Processing day of year  206\n",
      "Processing day of year  207\n",
      "Processing day of year  208\n",
      "Processing day of year  209\n",
      "Processing day of year  210\n",
      "Processing day of year  211\n",
      "Processing day of year  212\n",
      "Processing day of year  213\n",
      "Processing day of year  214\n",
      "Processing day of year  215\n",
      "Processing day of year  216\n",
      "Processing day of year  217\n",
      "Processing day of year  218\n",
      "Processing day of year  219\n",
      "Processing day of year  220\n",
      "Processing day of year  221\n",
      "Processing day of year  222\n",
      "Processing day of year  223\n",
      "Processing day of year  224\n",
      "Processing day of year  225\n",
      "Processing day of year  226\n",
      "Processing day of year  227\n",
      "Processing day of year  228\n",
      "Processing day of year  229\n",
      "Processing day of year  230\n",
      "Processing day of year  231\n",
      "Processing day of year  232\n",
      "Processing day of year  233\n",
      "Processing day of year  234\n",
      "Processing day of year  235\n",
      "Processing day of year  236\n",
      "Processing day of year  237\n",
      "Processing day of year  238\n",
      "Processing day of year  239\n",
      "Processing day of year  240\n",
      "Processing day of year  241\n",
      "Processing day of year  242\n",
      "Processing day of year  243\n",
      "Processing day of year  244\n",
      "Processing day of year  245\n",
      "Processing day of year  246\n",
      "Processing day of year  247\n",
      "Processing day of year  248\n",
      "Processing day of year  249\n",
      "Processing day of year  250\n",
      "Processing day of year  251\n",
      "Processing day of year  252\n",
      "Processing day of year  253\n",
      "Processing day of year  254\n",
      "Processing day of year  255\n",
      "Processing day of year  256\n",
      "Processing day of year  257\n",
      "Processing day of year  258\n",
      "Processing day of year  259\n",
      "Processing day of year  260\n",
      "Processing day of year  261\n",
      "Processing day of year  262\n",
      "Processing day of year  263\n",
      "Processing day of year  264\n",
      "Processing day of year  265\n",
      "Processing day of year  266\n",
      "Processing day of year  267\n",
      "Processing day of year  268\n",
      "Processing day of year  269\n",
      "Processing day of year  270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day of year  271\n",
      "Processing day of year  272\n",
      "Processing day of year  273\n",
      "Processing day of year  274\n",
      "Processing day of year  275\n",
      "Processing day of year  276\n",
      "Processing day of year  277\n",
      "Processing day of year  278\n",
      "Processing day of year  279\n",
      "Processing day of year  280\n",
      "Processing day of year  281\n",
      "Processing day of year  282\n",
      "Processing day of year  283\n",
      "Processing day of year  284\n",
      "Processing day of year  285\n",
      "Processing day of year  286\n",
      "Processing day of year  287\n",
      "Processing day of year  288\n",
      "Processing day of year  289\n",
      "Processing day of year  290\n",
      "Processing day of year  291\n",
      "Processing day of year  292\n",
      "Processing day of year  293\n",
      "Processing day of year  294\n",
      "Processing day of year  295\n",
      "Processing day of year  296\n",
      "Processing day of year  297\n",
      "Processing day of year  298\n",
      "Processing day of year  299\n",
      "Processing day of year  300\n",
      "Processing day of year  301\n",
      "Processing day of year  302\n",
      "Processing day of year  303\n",
      "Processing day of year  304\n",
      "Processing day of year  305\n",
      "Processing day of year  306\n",
      "Processing day of year  307\n",
      "Processing day of year  308\n",
      "Processing day of year  309\n",
      "Processing day of year  310\n",
      "Processing day of year  311\n",
      "Processing day of year  312\n",
      "Processing day of year  313\n",
      "Processing day of year  314\n",
      "Processing day of year  315\n",
      "Processing day of year  316\n",
      "Processing day of year  317\n",
      "Processing day of year  318\n",
      "Processing day of year  319\n",
      "Processing day of year  320\n",
      "Processing day of year  321\n",
      "Processing day of year  322\n",
      "Processing day of year  323\n",
      "Processing day of year  324\n",
      "Processing day of year  325\n",
      "Processing day of year  326\n",
      "Processing day of year  327\n",
      "Processing day of year  328\n",
      "Processing day of year  329\n",
      "Processing day of year  330\n",
      "Processing day of year  331\n",
      "Processing day of year  332\n",
      "Processing day of year  333\n",
      "Processing day of year  334\n",
      "Processing day of year  335\n",
      "Processing day of year  336\n",
      "Processing day of year  337\n",
      "Processing day of year  338\n",
      "Processing day of year  339\n",
      "Processing day of year  340\n",
      "Processing day of year  341\n",
      "Processing day of year  342\n",
      "Processing day of year  343\n",
      "Processing day of year  344\n",
      "Processing day of year  345\n",
      "Processing day of year  346\n",
      "Processing day of year  347\n",
      "Processing day of year  348\n",
      "Processing day of year  349\n",
      "Processing day of year  350\n",
      "Processing day of year  351\n",
      "Processing day of year  352\n",
      "Processing day of year  353\n",
      "Processing day of year  354\n",
      "Processing day of year  355\n",
      "Processing day of year  356\n",
      "Processing day of year  357\n",
      "Processing day of year  358\n",
      "Processing day of year  359\n",
      "Processing day of year  360\n",
      "Processing day of year  361\n",
      "Processing day of year  362\n",
      "Processing day of year  363\n",
      "Processing day of year  364\n",
      "Processing day of year  365\n",
      "Saved /home/disk/sipn/nicway/data/obs/NSIDC_0079/alpha_agg/1990_2017_Alpha.nc\n"
     ]
    }
   ],
   "source": [
    "update = False  # takes 5 min or so\n",
    "# only uses data through 2017\n",
    "# so only need to do this if change the smoothing length above or something else \n",
    "# about method\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')  # 0/0 made a lot of warnings that slowed us down\n",
    "\n",
    "start_year=1990\n",
    "Y_Start = start_year\n",
    "end_year = 2017  \n",
    "Y_End = end_year\n",
    "\n",
    "if update:\n",
    "\n",
    "    cyear = 1990 # all this is just to make an alpha dataarray to populate below\n",
    "    file_in = os.path.join(E.obs_dir, c_product,'sipn_nc_yearly_agg_anom', 'RegionalExtentsAnomalies_'+str(cyear)+'.nc')\n",
    "    ds = xr.open_mfdataset(file_in, autoclose=True, parallel=True)\n",
    "    alpha = expand_dimensions(0.*ds.Extent,fore_time=np.arange(1,366))\n",
    "    alpha.name = 'alpha'\n",
    "    alpha = alpha.drop('year')\n",
    "    alpha = alpha.drop('doy')\n",
    "    alpha.coords['init_time'] = xr.DataArray(alpha.fore_time.values, dims='time', coords={'time':alpha.time})\n",
    "    alpha = alpha.swap_dims({'time':'init_time'})\n",
    "    alpha = alpha.drop('time')\n",
    "    alpha = alpha.transpose('init_time', 'fore_time', 'nregions')\n",
    "    alpha = alpha.chunk({'init_time':1, 'fore_time': -1, 'nregions': -1})\n",
    "    ds = None\n",
    "\n",
    "    alpha.load()\n",
    "\n",
    "    file_in = os.path.join(E.obs_dir, c_product,'sipn_nc_yearly_agg_anom', 'RegionalExtentsAnomalies_*.nc')\n",
    "    allanoms = xr.open_mfdataset(file_in, autoclose=True, parallel=True)\n",
    "    allanoms = allanoms.Extent\n",
    "    allanoms = allanoms.swap_dims({'time':'year'})\n",
    "    allanoms = allanoms.sel(year=slice(Y_Start,Y_End))\n",
    "    allanoms = allanoms.where(allanoms.doy<=365, drop=True)  # get rid of leap days\n",
    "    allanoms = allanoms.chunk({'year':-1, 'nregions': -1})\n",
    "    allanoms = allanoms.drop('time')\n",
    "    print('allanoms ',allanoms)\n",
    "    allanoms.load()\n",
    "\n",
    "    # need to get rid of tiny numbers so that corrcoef isn't tiny/tiny = unexpected value\n",
    "    # with this it makes it 0/0 = NaN\n",
    "    allnp = allanoms.values\n",
    "    for creg in np.arange(0,14,1):\n",
    "        tmp = allnp[:,creg]\n",
    "        themax = np.amax(tmp)\n",
    "        print(creg, themax)\n",
    "        tmp[np.abs(tmp)<0.001*themax]=0.0\n",
    "        allnp[:,creg] = tmp\n",
    "    allanoms[:,:] = allnp\n",
    "    allnp = None\n",
    "\n",
    "    import xskillscore as xs\n",
    "\n",
    "    #for cdoy in np.arange(10,11,1):\n",
    "    for cdoy in np.arange(1,366,1):\n",
    "\n",
    "        print(\"Processing day of year \", cdoy)\n",
    "\n",
    "        # Select the initial week \n",
    "        thisday = allanoms.where(allanoms.doy==cdoy, drop=True)\n",
    "        #print(thisday)\n",
    "\n",
    "        for lag in np.arange(cdoy+1,366,1):\n",
    "            da_cdoy_lag = allanoms.where(allanoms.doy==lag, drop=True)\n",
    "            alpha[cdoy-1,lag-cdoy-1] = xs.pearson_r(thisday, da_cdoy_lag, 'year')\n",
    "            #print(lag-cdoy-1)\n",
    "\n",
    "        if cdoy>0:\n",
    "            # Select the initial week for one year less than full for fore_time in \n",
    "            # year after init_time\n",
    "            thisday = thisday.sel(year=slice(Y_Start,Y_End-1))\n",
    "\n",
    "            for lag in np.arange(366,cdoy+366,1):\n",
    "                da_cdoy_lag = allanoms.where(allanoms.doy==lag-365, drop=True)\n",
    "                da_cdoy_lag = da_cdoy_lag.sel(year=slice(Y_Start+1,Y_End))\n",
    "    #            print(thisday['year'].values)\n",
    "    #            print(da_cdoy_lag['year'].values)\n",
    "                #print(lag-cdoy-1)\n",
    "                da_cdoy_lag['year'].values = thisday['year'].values # year range must be identical to send to pearson_r tho shifted in actuality\n",
    "                alpha[cdoy-1,lag-cdoy-1] = xs.pearson_r(thisday, da_cdoy_lag, 'year')\n",
    "           \n",
    "    alpha = alpha.fillna(0)\n",
    "\n",
    "    file_out = os.path.join(E.obs_dir, c_product,'alpha_agg', str(Y_Start)+'_'+str(Y_End)+'_Alpha.nc')\n",
    "    alpha.to_netcdf(file_out)\n",
    "    print(\"Saved\",file_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
