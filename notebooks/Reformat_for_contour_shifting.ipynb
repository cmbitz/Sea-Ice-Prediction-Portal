{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import struct\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime \n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ESIO Imports\n",
    "\n",
    "from esio import EsioData as ed\n",
    "from esio import ice_plot\n",
    "from esio import metrics\n",
    "from esio import import_data\n",
    "\n",
    "import dask\n",
    "#dask.set_options(get=dask.threaded.get)\n",
    "# from dask.distributed import Client, progress\n",
    "# client = Client(processes=12)\n",
    "\n",
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "#############################################################\n",
    "# Load in Data\n",
    "#############################################################\n",
    "E = ed.EsioData.load()\n",
    "data_dir = E.data_dir\n",
    "grid_dir = E.grid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runType = 'reforecast'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to plot\n",
    "models_2_plot = list(E.model.keys())\n",
    "models_2_plot = [x for x in models_2_plot if x!='piomas'] # remove some models\n",
    "models_2_plot = [x for x in models_2_plot if E.icePredicted[x]] # Only predictive models\n",
    "models_2_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_2_plot = ['gfdlsipn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIC thresshold\n",
    "sic_threshold = 0.15 # Need to reduce model here to binary, because default dataType in CS is \"simple\" 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert netcdf file format into Contour Shifting format\n",
    "# \n",
    "# years x months x lat x lon\n",
    "#\n",
    "# Where years is actualy different initilzation days\n",
    "# and months is actually forecast lead time days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING! May need to change NSIDC data set depending on the model range used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Observations\n",
    "obs_f = '/home/disk/sipn/nicway/data/obs/NSIDC_0079/sipn_nc_yearly/*.nc'\n",
    "ds_obs = xr.open_mfdataset(obs_f, concat_dim='time', autoclose=True)\n",
    "# Format per Contour shifting (CS) req\n",
    "# values betweeen 0 and 100 indicate the sea ice concentration percentage, \n",
    "# values of 110 indicate the grid box is within the satellite hole, \n",
    "# and values of 120 indicate the grid box is on land.\n",
    "obs_CS = ds_obs.sic * 100 # fraction to percent\n",
    "obs_CS = obs_CS.where(obs_CS<=100, other = 120) # Set hole and land to 120\n",
    "obs_CS = obs_CS.where(ds_obs.hole_mask==0) # Set hole to Nan\n",
    "obs_CS = obs_CS.where(obs_CS<=120, other = 110) # Set nan (hole) to 110\n",
    "obs_CS = obs_CS.drop('hole_mask')\n",
    "obs_CS.name = 'conc'\n",
    "obs_CS.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONTHLY DATA\n",
    "\n",
    "# For each model\n",
    "for (i, cmod) in enumerate(models_2_plot):\n",
    "    print(cmod)\n",
    "    \n",
    "    # Load in Model\n",
    "    all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*.nc') \n",
    "    # Check we have files \n",
    "    files = glob.glob(all_files)\n",
    "    if not files:\n",
    "        #print(\"Skipping model\", cmod, \"no forecast files found.\")\n",
    "        continue # Skip this model\n",
    "    ds_mod_all = xr.open_mfdataset(files, concat_dim='init_time', autoclose=True, parallel=True)\n",
    "    \n",
    "    # Reduce to binary ice presence\n",
    "    ds_mod_all['sic'] = (ds_mod_all.sic >= sic_threshold).where(ds_mod_all.sic.isel(init_time=0,fore_time=0).notnull())\n",
    "    \n",
    "    # Only process monthly data\n",
    "    if (ds_mod_all.fore_offset[0].item()!='month'):\n",
    "        print(\"Not monthly, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Loop over years\n",
    "    for (cmonth, ds_mod) in ds_mod_all.groupby('init_time.month'):\n",
    "        if cmonth<11:\n",
    "            continue\n",
    "        print(cmonth)\n",
    "        \n",
    "        # Adjust Model format\n",
    "        # Want: (lat: 448, lon: 304, months: 12, years: 35)\n",
    "        print(\"Taking mean of ensemble to correct...\")\n",
    "        mod_CS = import_data.get_valid_time(ds_mod)\n",
    "        mod_CS = mod_CS.mean(dim='ensemble').rename({'fore_time':'months','init_time':'years','lat':'latitude','lon':'longitude'}).sic\n",
    "        mod_CS = mod_CS.rename({'nj':'lon','ni':'lat'})\n",
    "        mod_CS.coords['lon'] = np.arange(1,mod_CS.lon.size+1,1)\n",
    "        mod_CS.coords['lat'] = np.arange(1,mod_CS.lat.size+1,1)\n",
    "        mod_CS.name = 'iceInd'\n",
    "        mod_CS.coords['years'] = pd.to_datetime(mod_CS.years.values).year # Define year at an int\n",
    "\n",
    "        print(mod_CS.dims)\n",
    "\n",
    "        # Loop through each model \"init_time/years\", find observed time for each valid time\n",
    "        obs_CS_list = []\n",
    "        for it in mod_CS.years: # For each init time\n",
    "            temp_list = []\n",
    "            for ft in mod_CS.months: # For each forecast period\n",
    "                c_vt_S = mod_CS.valid_time.sel(years=it, months=ft).values # current valid time start\n",
    "                c_vt_E = c_vt_S.astype('M8[D]').astype('O') + relativedelta(months=1) - relativedelta(days=1) # add 1 month, minus one day (slice is inclusive)\n",
    "                c_obs = obs_CS.sel(time=slice(c_vt_S,c_vt_E))\n",
    "                #print(\"Found\",c_obs.time.size, \"days in month start\",c_vt_S)\n",
    "                if c_obs.time.size>0: # we found some time\n",
    "                    c_obs = c_obs.mean(dim='time')\n",
    "                    c_obs.coords['months'] = ft\n",
    "                    temp_list.append(c_obs)\n",
    "                else:\n",
    "                    print(\"Did not find month\", c_vt_S)\n",
    "                    xr.exit()\n",
    "            if len(temp_list)>0: # If we found any obs for current forecast valid times\n",
    "                da_temp = xr.concat(temp_list, dim='months')\n",
    "                da_temp.coords['years'] = it    \n",
    "                obs_CS_list.append(da_temp)\n",
    "        obs_CS_new = xr.concat(obs_CS_list, dim='years')\n",
    "\n",
    "        # Discard those forecasts (init_times/years) with any missing observations\n",
    "        # TODO: allow these later once Contour shift can handle them\n",
    "        OK_years = obs_CS_new.notnull().sum(dim=['x','y','months'])\n",
    "        OK_years = OK_years==OK_years.max().values\n",
    "        obs_CS_new = obs_CS_new.where(OK_years, drop=True)\n",
    "        mod_CS = mod_CS.where(OK_years, drop=True)\n",
    "        \n",
    "        # Add valid_time so we remember what the times mean!\n",
    "        #obs_CS_new.coords['valid_time'] = obs_CS_new.years + obs_CS_new.months\n",
    "        obs_CS_new = obs_CS_new.rename({'lat':'latitude','lon':'longitude'})\n",
    "        obs_CS_new = obs_CS_new.rename({'x':'lon','y':'lat'})\n",
    "        obs_CS_new.coords['lon'] = np.arange(1,obs_CS_new.lon.size+1,1)\n",
    "        obs_CS_new.coords['lat'] = np.arange(1,obs_CS_new.lat.size+1,1)\n",
    "\n",
    "        # drop extra vars\n",
    "        obs_CS_new = obs_CS_new.drop(['xm','ym'])\n",
    "        obs_CS_new = obs_CS_new.to_dataset()\n",
    "        mod_CS.name = 'iceInd'\n",
    "        mod_CS = mod_CS.to_dataset()\n",
    "        \n",
    "        obs_CS_new.coords['months'] = np.arange(1,obs_CS_new.months.size+1,1)\n",
    "        mod_CS.coords['months'] = np.arange(1,mod_CS.months.size+1,1)\n",
    "\n",
    "        # Reshape\n",
    "        obs_CS_new = obs_CS_new.transpose('lat','lon','months','years')\n",
    "        mod_CS = mod_CS.transpose('lat','lon','months','years')\n",
    "        \n",
    "        # Make smaller by downgrading actual type \n",
    "        obs_CS_new['conc'] = obs_CS_new.conc.astype('int16') # 0-120, so int16 is fine\n",
    "        mod_CS['iceInd'] = mod_CS.iceInd.astype('float32') # 0-1 as a fraction, so float32 is fine\n",
    "\n",
    "        # \"flip\" lat/y coord to match contour format\n",
    "        print(\"Flipping...\")\n",
    "        obs_CS_new['conc'] = xr.DataArray(np.flip(obs_CS_new.conc.values, axis=0), dims = obs_CS_new.conc.dims, coords = obs_CS_new.conc.coords)\n",
    "        mod_CS['iceInd'] = xr.DataArray(np.flip(mod_CS.iceInd.values, axis=0), dims = mod_CS.iceInd.dims, coords = mod_CS.iceInd.coords)\n",
    "\n",
    "        # Write to netcdf\n",
    "        out_dir = os.path.join(E.model_dir, cmod, runType, 'CS_yearly_monthly')\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        print(\"Writing to disk...\")\n",
    "        obs_CS_new.to_netcdf(os.path.join(out_dir, str(cmonth)+'_Obs_monthly.nc'))\n",
    "        mod_CS.to_netcdf(os.path.join(out_dir, str(cmonth)+'_Mod_monthly.nc'))\n",
    "\n",
    "        print(\"Finished year\", cmonth)\n",
    "    print(\"Finished model\", cmod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAILY FORECAST DATA (init interval, and forecast interval)\n",
    "\n",
    "# For each model\n",
    "for (i, cmod) in enumerate(models_2_plot):\n",
    "    print(cmod)\n",
    "    \n",
    "    # Load in Model\n",
    "    all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*.nc') \n",
    "    # Check we have files \n",
    "    files = glob.glob(all_files)\n",
    "    if not files:\n",
    "        #print(\"Skipping model\", cmod, \"no forecast files found.\")\n",
    "        continue # Skip this model\n",
    "    ds_mod_all = xr.open_mfdataset(files, concat_dim='init_time', autoclose=True, parallel=True)\n",
    "    \n",
    "    # Check we only handel models with fore_time as timedelta64\n",
    "    if (ds_mod_all.fore_time.dtype!='timedelta64[ns]'):\n",
    "        print(\"Model\",cmod,\"has fore_time with dtype not of timedelta64['ns'], likely by month, use other function\")\n",
    "        continue\n",
    "    \n",
    "    # Loop over years\n",
    "    for (cyear, ds_mod) in ds_mod_all.groupby('init_time.year'):\n",
    "        print(cyear)\n",
    "\n",
    "        # Adjust Model format\n",
    "        # Want: (lat: 448, lon: 304, months: 12, years: 35)\n",
    "        print(\"Taking mean of ensemble to correct...\")\n",
    "        mod_CS = ds_mod.mean(dim='ensemble').rename({'fore_time':'months','init_time':'years','lat':'latitude','lon':'longitude'}).sic\n",
    "        mod_CS = mod_CS.rename({'nj':'lon','ni':'lat'})\n",
    "        mod_CS.coords['lon'] = np.arange(1,mod_CS.lon.size+1,1)\n",
    "        mod_CS.coords['lat'] = np.arange(1,mod_CS.lat.size+1,1)\n",
    "        mod_CS.name = 'iceInd'\n",
    "        mod_CS.coords['valid_time'] = mod_CS.years + mod_CS.months\n",
    "        print(mod_CS.dims)\n",
    "\n",
    "        # Loop through each model \"init_time/years\", find observed time for each valid time\n",
    "        obs_CS_list = []\n",
    "        for it in mod_CS.years: # For each init time\n",
    "            temp_list = []\n",
    "            for ft in mod_CS.months: # For each forecast period\n",
    "                if (it+ft).values in obs_CS.time.values:\n",
    "                    c_obs = obs_CS.sel(time=it+ft)\n",
    "                    c_obs.coords['months'] = ft\n",
    "                    temp_list.append(c_obs)\n",
    "            if len(temp_list)>0: # If we found any obs for current forecast valid times\n",
    "                da_temp = xr.concat(temp_list, dim='months')\n",
    "                da_temp.coords['years'] = it    \n",
    "                obs_CS_list.append(da_temp)\n",
    "        obs_CS_new = xr.concat(obs_CS_list, dim='years')\n",
    "        print(obs_CS_new.dims)\n",
    "\n",
    "        # Discard those forecasts (init_times/years) with any missing observations\n",
    "        # TODO: allow these later once Contour shift can handle them\n",
    "        OK_years = obs_CS_new.notnull().sum(dim=['x','y','months'])\n",
    "        OK_years = OK_years==OK_years.max().values\n",
    "        obs_CS_new = obs_CS_new.where(OK_years, drop=True)\n",
    "        mod_CS = mod_CS.where(OK_years, drop=True)\n",
    "\n",
    "        # Add valid_time so we remember what the times mean!\n",
    "        obs_CS_new.coords['valid_time'] = obs_CS_new.years + obs_CS_new.months\n",
    "        obs_CS_new = obs_CS_new.rename({'lat':'latitude','lon':'longitude'})\n",
    "        obs_CS_new = obs_CS_new.rename({'x':'lon','y':'lat'})\n",
    "        obs_CS_new.coords['lon'] = np.arange(1,obs_CS_new.lon.size+1,1)\n",
    "        obs_CS_new.coords['lat'] = np.arange(1,obs_CS_new.lat.size+1,1)\n",
    "\n",
    "        # Set coords years and months to simple indexs for R code\n",
    "        obs_CS_new.coords['years'] = np.arange(1,obs_CS_new.years.size+1,1)\n",
    "        mod_CS.coords['years'] = np.arange(1,obs_CS_new.years.size+1,1)\n",
    "        obs_CS_new.coords['months'] = np.arange(1,obs_CS_new.months.size+1,1)\n",
    "        mod_CS.coords['months'] = np.arange(1,obs_CS_new.months.size+1,1)\n",
    "\n",
    "        # drop extra vars\n",
    "        obs_CS_new = obs_CS_new.drop(['xm','ym','time'])\n",
    "        obs_CS_new = obs_CS_new.to_dataset()\n",
    "        mod_CS.name = 'iceInd'\n",
    "        mod_CS = mod_CS.to_dataset()\n",
    "\n",
    "        # Reshape\n",
    "        obs_CS_new = obs_CS_new.transpose('lat','lon','months','years')\n",
    "        mod_CS = mod_CS.transpose('lat','lon','months','years')\n",
    "        \n",
    "        # Make smaller by downgrading actual type \n",
    "        obs_CS_new['conc'] = obs_CS_new.conc.astype('int16') # 0-120, so int16 is fine\n",
    "        mod_CS['iceInd'] = mod_CS.iceInd.astype('float32') # 0-1 as a fraction, so float32 is fine\n",
    "\n",
    "        # \"flip\" lat/y coord to match contour format\n",
    "        print(\"Flipping...\")\n",
    "        obs_CS_new['conc'] = xr.DataArray(np.flip(obs_CS_new.conc.values, axis=0), dims = obs_CS_new.conc.dims, coords = obs_CS_new.conc.coords)\n",
    "        mod_CS['iceInd'] = xr.DataArray(np.flip(mod_CS.iceInd.values, axis=0), dims = mod_CS.iceInd.dims, coords = mod_CS.iceInd.coords)\n",
    "\n",
    "        # Write to netcdf\n",
    "        out_dir = os.path.join(E.model_dir, cmod, runType, 'CS_daily')\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        print(\"Writing to disk...\")\n",
    "        obs_CS_new.to_netcdf(os.path.join(out_dir, str(cyear)+'_Obs_daily.nc'))\n",
    "        mod_CS.to_netcdf(os.path.join(out_dir, str(cyear)+'_Mod_daily.nc'))\n",
    "\n",
    "        print(\"Finished year\", cyear)\n",
    "    print(\"Finished model\", cmod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_CS_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Reformat into original Contour Shifting format (one file per each DOY initilization)\n",
    "# For each model\n",
    "for (i, cmod) in enumerate(models_2_plot):\n",
    "    print(cmod)\n",
    "    \n",
    "    for prefix in ['Mod','Obs']:\n",
    "        print(prefix)\n",
    "        # Load in\n",
    "        all_files = os.path.join(E.model_dir, cmod, runType, 'CS_daily', '*'+prefix+'*.nc') \n",
    "        # Check we have files \n",
    "        files = glob.glob(all_files)\n",
    "        if not files:\n",
    "            continue # Skip this model\n",
    "        ds_mod_all = xr.open_mfdataset(sorted(files),  \n",
    "                                       chunks={'lat':448, 'lon': 304, 'months': 43, 'years': 1},\n",
    "                                       concat_dim='realYear', autoclose=True, parallel=True)\n",
    "        if prefix=='Obs':\n",
    "            ds_mod_all['conc'] = ds_mod_all.conc.astype('int16')\n",
    "            \n",
    "        # Create output dir\n",
    "        out_dir = os.path.join(E.model_dir, cmod, runType, 'CS_yearly')\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        # Loop over each \"year\" (really the DOY for each year) and write to disk\n",
    "        for cdoy in ds_mod_all.years.values:\n",
    "            print(cdoy)\n",
    "            ds_cdoy = ds_mod_all.sel(years=cdoy).drop('years')\n",
    "            ds_cdoy = ds_cdoy.rename({'realYear':'years'})\n",
    "            ds_cdoy.coords['years'] = np.arange(1,ds_cdoy.years.size+1)\n",
    "            ds_cdoy = ds_cdoy.transpose('lat','lon','months','years')\n",
    "            # TODO check for NaN?\n",
    "\n",
    "            ds_cdoy.to_netcdf(os.path.join(out_dir, str(cdoy)+'_'+prefix+'_daily.nc'))\n",
    "        ds_mod_all = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Reformat into original Contour Shifting format (one file per each DOY initilization)\n",
    "# For each model\n",
    "for (i, cmod) in enumerate(models_2_plot):\n",
    "    print(cmod)\n",
    "    \n",
    "    for prefix in ['Mod','Obs']:\n",
    "        print(prefix)\n",
    "        # Load in\n",
    "        all_files = os.path.join(E.model_dir, cmod, runType, 'CS_daily', '*'+prefix+'*.nc') \n",
    "        # Check we have files \n",
    "        files = glob.glob(all_files)\n",
    "        if not files:\n",
    "            continue # Skip this model\n",
    "        ds_mod_all = xr.open_mfdataset(sorted(files),  \n",
    "                                       chunks={'lat':448, 'lon': 304, 'months': 43, 'years': 1},\n",
    "                                       concat_dim='realYear', autoclose=True, parallel=True)\n",
    "        if prefix=='Obs':\n",
    "            ds_mod_all['conc'] = ds_mod_all.conc.astype('int16')\n",
    "            \n",
    "        # Create output dir\n",
    "        out_dir = os.path.join(E.model_dir, cmod, runType, 'CS_yearly_weekly_mean')\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            \n",
    "        # Aggregate to weekly means\n",
    "        y_bins = np.arange(0,ds_mod_all.years.size,7)\n",
    "        y_bin_labels = y_bins[1:]\n",
    "        ds_MON = ds_mod_all.groupby_bins('years', y_bins, labels=y_bin_labels).mean(dim='years')\n",
    "\n",
    "        # Loop over each \"year\" (really the WOY (Week of year) for each year) and write to disk\n",
    "        for cwoy in ds_MON.years_bins.values:\n",
    "            print(cwoy)\n",
    "            ds_cwoy = ds_MON.sel(years_bins=cwoy).drop('years_bins')\n",
    "            ds_cwoy = ds_cwoy.rename({'realYear':'years'})\n",
    "            ds_cwoy.coords['years'] = np.arange(1,ds_cwoy.years.size+1)\n",
    "            ds_cwoy = ds_cwoy.transpose('lat','lon','months','years')\n",
    "            # TODO check for NaN?\n",
    "\n",
    "            ds_cwoy.to_netcdf(os.path.join(out_dir, str(cwoy)+'_'+prefix+'_weekly.nc'))\n",
    "        ds_mod_all = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Reformat into original Contour Shifting format (one file per each DOY initilization)\n",
    "# For each model\n",
    "for (i, cmod) in enumerate(models_2_plot):\n",
    "    print(cmod)\n",
    "    \n",
    "    for prefix in ['Mod','Obs']:\n",
    "        print(prefix)\n",
    "        # Load in\n",
    "        all_files = os.path.join(E.model_dir, cmod, runType, 'CS_daily', '*'+prefix+'*.nc') \n",
    "        # Check we have files \n",
    "        files = glob.glob(all_files)\n",
    "        if not files:\n",
    "            continue # Skip this model\n",
    "        ds_mod_all = xr.open_mfdataset(sorted(files),  \n",
    "                                       chunks={'lat':448, 'lon': 304, 'months': 43, 'years': 1},\n",
    "                                       concat_dim='realYear', autoclose=True, parallel=True)\n",
    "        if prefix=='Obs':\n",
    "            ds_mod_all['conc'] = ds_mod_all.conc.astype('int16')\n",
    "            \n",
    "        # Create output dir\n",
    "        out_dir = os.path.join(E.model_dir, cmod, runType, 'CS_yearly_monthly_mean')\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            \n",
    "        # Aggregate to monthly means\n",
    "        # First, years is just an int representing the day of the year\n",
    "        # Convert to temp np.datetime64, to allow us to group it by months\n",
    "        orig_years = ds_mod_all.years # Save old years coord\n",
    "        # Add new datetime64 for a temp year\n",
    "        ds_mod_all['years'] = pd.date_range(start='2000-01-01', periods=ds_mod_all.years.size, freq='D')\n",
    "        # Group by month\n",
    "        ds_MON = ds_mod_all.groupby('years.month').mean(dim='years')\n",
    "        # years should now be the month of the year (int??)\n",
    "        ds_MON = ds_MON.rename({'realYear':'years'})\n",
    "        ds_MON.coords['years'] = np.arange(1,ds_MON.years.size+1)\n",
    "\n",
    "        # Loop over each init month\n",
    "        for cmon in ds_MON.month.values: # each init month\n",
    "            print(cmon)\n",
    "            ds_cwoy = ds_MON.sel(month=cmon)\n",
    "            ds_cwoy = ds_cwoy.transpose('lat','lon','months','years')\n",
    "            # TODO check for NaN?\n",
    "\n",
    "            ds_cwoy.to_netcdf(os.path.join(out_dir, str(cmon)+'_'+prefix+'_monthly.nc'))\n",
    "        ds_mod_all = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.4 esio",
   "language": "python",
   "name": "esio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
