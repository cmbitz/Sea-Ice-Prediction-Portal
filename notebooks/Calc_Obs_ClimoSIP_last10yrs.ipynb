{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "For the bootstrap and latest nrt weekly mean obs since 1990 (it was made weekly in Agg_NSIDC_Obs)\n",
    "filter with a LOESS smoother in time then polynomial fit to get the \n",
    "fit parameters. Save the fit parameters since this takes forever.\n",
    "\n",
    "Later read in fit parameters to extrapolate forward, giving climatological trend \n",
    "benchmark\n",
    "\n",
    "Also use fit parameters for each year to compute an anomaly and save that too for computing alpha\n",
    "\n",
    "At present this routine is meant to be used once a year, but should make it so that it produces a new climotrend\n",
    "estimate each week!!!\n",
    "\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from esio import EsioData as ed\n",
    "from esio import ice_plot\n",
    "from esio import import_data\n",
    "from esio import metrics\n",
    "import dask\n",
    "\n",
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=.8, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climatology SIP is average presence for the past 10 years after first computing weekly means starting with Jan 1 each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x14c948088da0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dask.distributed import Client\n",
    "# client = Client(n_workers=2)\n",
    "# client = Client()\n",
    "# client\n",
    "dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler\n",
    "# dask.config.set(scheduler='processes')  # overwrite default with threaded scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "today_date = datetime.datetime.now()\n",
    "nextyear = today_date.year + 1\n",
    "E = ed.EsioData.load()\n",
    "mod_dir = E.model_dir\n",
    "cmod = 'climoSIP'\n",
    "runType = 'forecast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 2020\n",
      "The last week to compute is  28\n",
      "<xarray.DataArray 'SIP' (time: 28, y: 448, x: 304)>\n",
      "dask.array<shape=(28, 448, 304), dtype=float64, chunksize=(1, 448, 304)>\n",
      "Coordinates:\n",
      "  * x        (x) int64 0 1 2 3 4 5 6 7 8 ... 295 296 297 298 299 300 301 302 303\n",
      "  * y        (y) int64 0 1 2 3 4 5 6 7 8 ... 439 440 441 442 443 444 445 446 447\n",
      "    lat      (x, y) float64 31.1 31.25 31.4 31.55 ... 34.92 34.77 34.62 34.47\n",
      "    lon      (x, y) float64 168.3 168.4 168.5 168.7 ... -9.745 -9.872 -9.999\n",
      "    xm       (x) int64 -3850000 -3825000 -3800000 ... 3675000 3700000 3725000\n",
      "    ym       (y) int64 5850000 5825000 5800000 ... -5275000 -5300000 -5325000\n",
      "    week     (time) int64 1 2 3 4 5 6 7 8 9 10 ... 19 20 21 22 23 24 25 26 27 28\n",
      "  * time     (time) datetime64[ns] 2020-01-04 2020-01-11 ... 2020-07-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/core.py:137: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return func(*args2)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.divide(x1, x2, out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /home/disk/sipn/nicway/data/model/climoSIP/forecast/sipn_nc_yearly_byweek/2020_byweek.nc\n"
     ]
    }
   ],
   "source": [
    "#for pred_year in np.arange(1995,2020,1):   # done to make reforecast\n",
    "for pred_year in [nextyear]:\n",
    "    start_year = pred_year - 10\n",
    "    print(start_year,pred_year)  # normally we are computing the fits for predicting far into the future\n",
    "\n",
    "    #############################################################\n",
    "    # Load in Data that have already been averaged for each week of the year, always starting on Jan 1\n",
    "    #############################################################\n",
    "\n",
    "    # BE SURE THESE ARE NON OVERLAPPING, MUST BE UPDATED FOR NEW DATA\n",
    "    # Get bootstrap and nrt observations with pole hole already filled in\n",
    "    ds_81 = xr.open_mfdataset(E.obs['NSIDC_0081']['sipn_nc']+'_yearly_byweek/*byweek.nc', concat_dim='time', autoclose=True, parallel=True).sic\n",
    "    #ds_51 = xr.open_mfdataset(E.obs['NSIDC_0051']['sipn_nc']+'_yearly_byweek/*byweek.nc', concat_dim='time', autoclose=True, parallel=True)\n",
    "    ds_79 = xr.open_mfdataset(E.obs['NSIDC_0079']['sipn_nc']+'_yearly_byweek/*byweek.nc', concat_dim='time', autoclose=True, parallel=True).sic\n",
    "\n",
    "    ds_79=ds_79.sel(time=slice(str(start_year),str(pred_year-1)))  # end year just has to be way in the future\n",
    "    ds_81=ds_81.sel(time=slice('2015',str(pred_year-1)))  # restrict to before prediciton year, lower year not important\n",
    "\n",
    "    # Combine bootstrap with NASA NRT\n",
    "    da_sic = ds_79.combine_first(ds_81)  # takes ds_79 as priority\n",
    "    ds_79 = None\n",
    "    ds_81 = None\n",
    "\n",
    "    #da_sic=ds_81  # for testing\n",
    "    # add year coordinate\n",
    "    year_all = [x.year for x in pd.to_datetime(da_sic.time.values)]\n",
    "    da_sic.coords['year'] = xr.DataArray(year_all, dims='time', coords={'time':da_sic.time})\n",
    "\n",
    "    # put week coordinate back since combine first rubbed them out\n",
    "    DOY = [x.timetuple().tm_yday for x in pd.to_datetime(da_sic.time.values)]\n",
    "    weeks= np.ceil(np.divide(DOY,7))\n",
    "    weeks = weeks.astype(int)\n",
    "    da_sic.coords['week'] = xr.DataArray(weeks, dims='time', coords={'time':da_sic.time})\n",
    "    #print(da_sic)\n",
    "\n",
    "    # plot so we are sure this is going right\n",
    "    ocnmask = da_sic.isel(time=-30).notnull()  # take a value near the end when not likely to have missing values\n",
    "    ocnmask.name = 'oceanmask'\n",
    "\n",
    "    maxweek = da_sic.sel(time=str(pred_year-1)).week.values[-1]\n",
    "    print('The last week to compute is ',maxweek)\n",
    "\n",
    "    # Convert sea ice presence\n",
    "    ds_sp = (da_sic >= 0.15).astype('int') # This unfortunatly makes all NaN -> zeros...\n",
    "    ds_sp = ds_sp.where(ocnmask,other=np.nan)  \n",
    "    ds_sp.coords['week'] = da_sic.week\n",
    "    #print(ds_sp)\n",
    "\n",
    "    # Calculate mean SIP\n",
    "    ds_sip_climo = ds_sp.groupby('week').mean(dim='time')\n",
    "    ds_sip_climo = ds_sip_climo.sel(week=slice(1,maxweek))\n",
    "\n",
    "    ds_sip_climo.name = 'SIP'\n",
    "    ds_sip_climo.coords['time'] = xr.DataArray([datetime.datetime(pred_year,1,1) + datetime.timedelta(days=int(7*(x-1)+3)) for x in np.arange(1,maxweek+1,1)], dims='week')\n",
    "    ds_sip_climo=ds_sip_climo.swap_dims({'week':'time'})\n",
    "    print(ds_sip_climo)\n",
    "\n",
    "    file_out = os.path.join(mod_dir, cmod, runType, 'sipn_nc_yearly_byweek',str(pred_year)+'_byweek.nc')\n",
    "    ds_sip_climo.to_netcdf(file_out)\n",
    "    print(\"Saved\",file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotTest = False\n",
    "if PlotTest:\n",
    "    tmpsip1=ds_sip_climo.sel(week=1) # save one time at random for plot verification\n",
    "    tmpsip2=ds_sip_climo.sel(week=maxweek) # save one time at random for plot verification\n",
    "\n",
    "    # plot one time at random to ensure it is about right Nplots has to be one more than you'd think\n",
    "    (f, axes) = ice_plot.multi_polar_axis(ncols=2, nrows=1, Nplots = 3, sizefcter=3)\n",
    "    tmpsip1.plot.pcolormesh(cmap='Reds',ax=axes[0], x='lon', y='lat',transform=ccrs.PlateCarree())\n",
    "    tmpsip2.plot.pcolormesh(cmap='Reds',ax=axes[1], x='lon', y='lat',transform=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_slices are at end of the range\n",
      "2018-01-07T00:00:00.000000000 2020-07-05T00:00:00.000000000\n",
      "2018-01-04T00:00:00.000000000 2020-07-02T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded start date (makes incremental weeks always the same)\n",
    "start_t = datetime.datetime(1950, 1, 1) # datetime.datetime(1950, 1, 1)\n",
    "# Params for this plot\n",
    "Ndays = 7 # time period to aggregate maps to (default is 7)\n",
    "\n",
    "init_start_date = np.datetime64('2018-01-01') # first date we have computed metrics\n",
    "                   \n",
    "#init_start_date = np.datetime64('2019-01-01') # speeds up substantially b\n",
    "\n",
    "cd = today_date +  datetime.timedelta(days=365)\n",
    "\n",
    "init_slice = np.arange(start_t, cd, datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "# init_slice = init_slice[-Npers:] # Select only the last Npers of periods (weeks) since current date\n",
    "init_slice = init_slice[init_slice>=init_start_date] # Select only the inits after init_start_date\n",
    "\n",
    "init_midpoint = np.arange(start_t- datetime.timedelta(days=3), cd- datetime.timedelta(days=3), datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "init_midpoint = init_midpoint[init_midpoint>=init_start_date] # Select only the inits after init_start_date\n",
    "\n",
    "print('init_slices are at end of the range')\n",
    "print(init_slice[0],init_slice[-1])\n",
    "print('shift to midpoint of range')\n",
    "print(init_midpoint[0],init_midpoint[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'SIP' (time: 132, y: 448, x: 304)>\n",
      "dask.array<shape=(132, 448, 304), dtype=float64, chunksize=(132, 8, 304)>\n",
      "Coordinates:\n",
      "  * x        (x) int64 0 1 2 3 4 5 6 7 8 ... 295 296 297 298 299 300 301 302 303\n",
      "  * y        (y) int64 0 1 2 3 4 5 6 7 8 ... 439 440 441 442 443 444 445 446 447\n",
      "    lat      (x, y) float64 dask.array<shape=(304, 448), chunksize=(304, 8)>\n",
      "    lon      (x, y) float64 dask.array<shape=(304, 448), chunksize=(304, 8)>\n",
      "    xm       (x) int64 dask.array<shape=(304,), chunksize=(304,)>\n",
      "    ym       (y) int64 dask.array<shape=(448,), chunksize=(8,)>\n",
      "    week     (time) int64 dask.array<shape=(132,), chunksize=(132,)>\n",
      "  * time     (time) datetime64[ns] 2018-01-04 2018-01-11 ... 2020-07-11\n",
      "<xarray.DataArray 'SIP' (time: 131, y: 448, x: 304)>\n",
      "dask.array<shape=(131, 448, 304), dtype=float64, chunksize=(131, 8, 304)>\n",
      "Coordinates:\n",
      "  * x        (x) int64 0 1 2 3 4 5 6 7 8 ... 295 296 297 298 299 300 301 302 303\n",
      "  * y        (y) int64 0 1 2 3 4 5 6 7 8 ... 439 440 441 442 443 444 445 446 447\n",
      "    lat      (x, y) float64 dask.array<shape=(304, 448), chunksize=(304, 8)>\n",
      "    lon      (x, y) float64 dask.array<shape=(304, 448), chunksize=(304, 8)>\n",
      "    xm       (x) int64 dask.array<shape=(304,), chunksize=(304,)>\n",
      "    ym       (y) int64 dask.array<shape=(448,), chunksize=(8,)>\n",
      "  * time     (time) datetime64[ns] 2018-01-07 2018-01-14 ... 2020-07-05\n",
      "Saved /home/disk/sipn/nicway/data/model/climoSIP/forecast/interpolated/climoSIP_2018_to_nextyear.nc\n"
     ]
    }
   ],
   "source": [
    "files = os.path.join(mod_dir, cmod, runType, 'sipn_nc_yearly_byweek','*_byweek.nc')\n",
    "da = xr.open_mfdataset(files, concat_dim='time', parallel=True).SIP\n",
    "cd = today_date +  datetime.timedelta(days=375)\n",
    "da=da.sel(time=slice(str(init_start_date),str(cd)))  # end year just has to be way in the future\n",
    "da=da.chunk({'time': 132, 'y': 8})\n",
    "print(da)\n",
    "dar=da.interp(time=init_midpoint,method= 'linear')\n",
    "dar['time']=init_slice\n",
    "dar = dar.drop('week')\n",
    "print(dar)\n",
    "file_out = os.path.join(mod_dir, cmod, runType, 'interpolated','climoSIP_2018_to_nextyear.nc')\n",
    "dar.to_netcdf(file_out)\n",
    "print(\"Saved\",file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotTest = False\n",
    "if PlotTest:\n",
    "    tmpsip1=da.isel(time=80) # save one time at random for plot verification\n",
    "    tmpsip2=dar.isel(time=80) # save one time at random for plot verification\n",
    "    tmpsip2=tmpsip2-tmpsip1\n",
    "\n",
    "    # plot one time at random to ensure it is about right Nplots has to be one more than you'd think\n",
    "    (f, axes) = ice_plot.multi_polar_axis(ncols=2, nrows=1, Nplots = 3, sizefcter=3)\n",
    "    tmpsip1.plot.pcolormesh(cmap='Reds',ax=axes[0], x='lon', y='lat',transform=ccrs.PlateCarree())\n",
    "    tmpsip2.plot.pcolormesh(cmap='Reds',ax=axes[1], x='lon', y='lat',transform=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
