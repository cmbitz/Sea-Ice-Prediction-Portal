{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Standard Imports\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "# ESIO Imports\n",
    "\n",
    "from esio import EsioData as ed\n",
    "from esio import import_data\n",
    "from esio import ice_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f8a2c33d438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler\n",
    "# client = Client(n_workers=8)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = ed.EsioData.load()\n",
    "# Directories\n",
    "all_models=['rasmesrl']\n",
    "runType='forecast'\n",
    "updateall = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (ni: 448, ni_b: 449, nj: 304, nj_b: 305)\n",
       "Coordinates:\n",
       "  * nj       (nj) int64 0 1 2 3 4 5 6 7 8 ... 296 297 298 299 300 301 302 303\n",
       "  * ni       (ni) int64 0 1 2 3 4 5 6 7 8 ... 440 441 442 443 444 445 446 447\n",
       "  * nj_b     (nj_b) int64 0 1 2 3 4 5 6 7 8 ... 297 298 299 300 301 302 303 304\n",
       "  * ni_b     (ni_b) int64 0 1 2 3 4 5 6 7 8 ... 441 442 443 444 445 446 447 448\n",
       "Data variables:\n",
       "    lat      (nj, ni) float64 31.1 31.25 31.4 31.55 ... 34.92 34.77 34.62 34.47\n",
       "    lon      (nj, ni) float64 168.3 168.4 168.5 168.7 ... -9.745 -9.872 -9.999\n",
       "    lat_b    (nj_b, ni_b) float64 31.13 31.28 31.42 31.57 ... 34.49 34.35 31.37\n",
       "    lon_b    (nj_b, ni_b) float64 168.5 168.6 168.7 ... -9.846 -9.972 102.3\n",
       "    imask    (nj, ni) bool True True True True True ... True True True True True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stero_grid_file = E.obs['NSIDC_0051']['grid']\n",
    "obs_grid = import_data.load_grid_info(stero_grid_file, model='NSIDC')\n",
    "# Ensure latitude is within bounds (-90 to 90)\n",
    "# Have to do this because grid file has 90.000001\n",
    "obs_grid['lat_b'] = obs_grid.lat_b.where(obs_grid.lat_b < 90, other = 90)\n",
    "obs_grid.rename({'imask':'mask'}, inplace=True);\n",
    "obs_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CAFS SIC (aice) has nan for all non-sea ice covered areas. So use the sst field to create the land mask\n",
    "# land_mask is the fraction of native grid cell that is land\n",
    "def get_land_mask_hack(ds):\n",
    "    ds_land_mask = ds.sst[0,:,:].drop('time')\n",
    "    ds_land_mask = ds_land_mask.isnull()\n",
    "    ds_land_mask.name = 'land_mask'\n",
    "    ds_land_mask.attrs = {'land_mask':'the fraction of native grid cell that is land'}\n",
    "    return ds_land_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaNOcean_with_Zeros(ds=None, vars=None, ds_land_mask=None):\n",
    "    ds_out = ds\n",
    "    for cvar in vars:\n",
    "        ds_out[cvar] = ds_out[cvar].fillna(0.0).where(~ds_land_mask)\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_small = ds.sst[:,0:10,0:10].rename({'lat':'nj','lon':'ni'})\n",
    "# ds_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(ds_small.ULON.values.flatten(), ds_small.ULAT.values.flatten(),'k*',label='U')\n",
    "# plt.plot(ds_small.TLON.values.flatten(), ds_small.TLAT.values.flatten(),'ro',label='T')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(ds.lat[0:10,0:10].values.flatten(), ds.lon[0:10,0:10].values.flatten(),'ro',label='center')\n",
    "# plt.plot(ds.lat_b[0:11,0:11].values.flatten(), ds.lon_b[0:11,0:11].values.flatten(),'ko',label='bounds')\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(obs_grid.lat.values.flatten(), obs_grid.lon.values.flatten(),'mo',label='center_OBS')\n",
    "# plt.plot(ds.lat.values.flatten(), ds.lon.values.flatten(),'ro',label='center')\n",
    "# plt.plot(ds.lat_b.values.flatten(), ds.lon_b.values.flatten(),'ko',label='bounds')\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# def get_lat_lon_bounds_from_corner(cen_lat=None, cen_lon=None):\n",
    "#     ''' Some models only provide lat lon coords or the cell center and the corners. Transform to\n",
    "#     the bounding N+1 lats'''\n",
    "\n",
    "#     # Input\n",
    "#     # Center lat and lon of grid cells (N x M)\n",
    "#     #\n",
    "#     # Output\n",
    "#     # lat_b and lon_b - bounds (N+1 x M+1) for each grid lat lon grid cell center\n",
    "\n",
    "#     # Add cell bound coords (lat_b and lon_b)\n",
    "#     n_j = cen_lat.nj.size\n",
    "#     n_i = cen_lat.ni.size\n",
    "#     nj_b = np.arange(0, n_j + 1) # indices of edge of cells\n",
    "#     ni_b = np.arange(0, n_i + 1)\n",
    "\n",
    "#     nj = np.arange(0, n_j)\n",
    "#     ni = np.arange(0, n_i)\n",
    "\n",
    "#     interf_lat = RegularGridInterpolator((nj, ni), cen_lat, bounds_error=False, fill_value=None)\n",
    "#     interf_lon = RegularGridInterpolator((nj, ni), cen_lon, bounds_error=False, fill_value=None)\n",
    "\n",
    "#     # Create empty matrix\n",
    "#     b_grid_lat = np.ones((n_j + 1, n_i + 1))*np.NaN\n",
    "#     b_grid_lon = np.ones((n_j + 1, n_i + 1))*np.NaN\n",
    "#     # Interpolate each value (inner only)\n",
    "#     for ci in ni_b:\n",
    "#         for cj in nj_b:\n",
    "#             b_grid_lat[cj,ci] = interf_lat([[cj-0.5, ci-0.5]])\n",
    "#             b_grid_lon[cj,ci] = interf_lon([[cj-0.5, ci-0.5]])\n",
    "\n",
    "#     ds_lat_b = xr.DataArray(b_grid_lat, dims=('nj_b', 'ni_b'), coords={'nj_b':nj_b, 'ni_b':ni_b})\n",
    "#     ds_lon_b = xr.DataArray(b_grid_lon, dims=('nj_b', 'ni_b'), coords={'nj_b':nj_b, 'ni_b':ni_b})\n",
    "\n",
    "#     return (ds_lat_b, ds_lon_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ds_lat_b, ds_lon_b) = get_lat_lon_bounds_from_corner(cen_lat=ds.rename({'lat':'nj','lon':'ni'}).TLAT[0:10,0:10], \n",
    "#                                                       cen_lon=ds.rename({'lat':'nj','lon':'ni'}).TLON[0:10,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(ds.TLON[0:10,0:10].values.flatten(), ds.TLAT[0:10,0:10].values.flatten(),'ro',label='center')\n",
    "# plt.plot(ds_lon_b.values.flatten(), ds_lat_b.values.flatten(),'ko',label='bounds')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regridding Options\n",
    "method='conservative_normed' # ['bilinear', 'conservative_normed', 'conservative', 'nearest_s2d', 'nearest_d2s', 'patch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# - Get mask\n",
    "# - Get lat lon bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dic = {'aice':'sic','lat':'nj','lon':'ni','TLAT':'lat','TLON':'lon'}\n",
    "var_dic_new = {'aice':'sic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regridding  rasmesrl ...\n",
      "Found  397  initialization times.\n",
      "Only updating new files\n",
      "Skipping  2016-10-28  already imported.\n",
      "Skipping  2016-10-29  already imported.\n",
      "Skipping  2016-10-30  already imported.\n",
      "Skipping  2016-10-31  already imported.\n",
      "Skipping  2016-11-01  already imported.\n",
      "Skipping  2016-11-02  already imported.\n",
      "Skipping  2016-11-03  already imported.\n",
      "Skipping  2016-11-04  already imported.\n",
      "Skipping  2016-11-05  already imported.\n",
      "Skipping  2016-11-06  already imported.\n",
      "Skipping  2016-11-07  already imported.\n",
      "Skipping  2016-11-08  already imported.\n",
      "Skipping  2016-11-10  already imported.\n",
      "Skipping  2016-11-11  already imported.\n",
      "Skipping  2016-11-12  already imported.\n",
      "Skipping  2016-11-13  already imported.\n",
      "Skipping  2016-11-14  already imported.\n",
      "Skipping  2016-11-15  already imported.\n",
      "Skipping  2016-11-16  already imported.\n",
      "Skipping  2016-11-17  already imported.\n",
      "Skipping  2016-11-18  already imported.\n",
      "Skipping  2016-11-19  already imported.\n",
      "Skipping  2017-07-27  already imported.\n",
      "Skipping  2017-07-29  already imported.\n",
      "Skipping  2017-07-31  already imported.\n",
      "Skipping  2017-08-04  already imported.\n",
      "Skipping  2017-08-05  already imported.\n",
      "Skipping  2017-08-06  already imported.\n",
      "Skipping  2017-08-07  already imported.\n",
      "Skipping  2017-08-08  already imported.\n",
      "Skipping  2017-08-09  already imported.\n",
      "Skipping  2017-08-10  already imported.\n",
      "Skipping  2017-08-11  already imported.\n",
      "Skipping  2017-08-12  already imported.\n",
      "Skipping  2017-08-13  already imported.\n",
      "Skipping  2017-08-14  already imported.\n",
      "Skipping  2017-08-15  already imported.\n",
      "Skipping  2017-08-16  already imported.\n",
      "Skipping  2017-08-17  already imported.\n",
      "Skipping  2017-08-18  already imported.\n",
      "Skipping  2017-08-19  already imported.\n",
      "Skipping  2017-08-20  already imported.\n",
      "Skipping  2017-08-21  already imported.\n",
      "Skipping  2017-08-22  already imported.\n",
      "Skipping  2017-08-23  already imported.\n",
      "Skipping  2017-08-24  already imported.\n",
      "Skipping  2017-08-25  already imported.\n",
      "Skipping  2017-08-26  already imported.\n",
      "Skipping  2017-08-27  already imported.\n",
      "Skipping  2017-08-28  already imported.\n",
      "Skipping  2017-08-29  already imported.\n",
      "Skipping  2017-08-30  already imported.\n",
      "Skipping  2017-08-31  already imported.\n",
      "Skipping  2017-09-01  already imported.\n",
      "Skipping  2017-09-02  already imported.\n",
      "Skipping  2017-09-03  already imported.\n",
      "Skipping  2017-09-04  already imported.\n",
      "Skipping  2017-09-05  already imported.\n",
      "Skipping  2017-09-06  already imported.\n",
      "Skipping  2017-09-09  already imported.\n",
      "Skipping  2017-09-10  already imported.\n",
      "Skipping  2017-09-13  already imported.\n",
      "Skipping  2017-09-14  already imported.\n",
      "Skipping  2017-09-15  already imported.\n",
      "Skipping  2017-09-16  already imported.\n",
      "Skipping  2017-09-17  already imported.\n",
      "Skipping  2017-09-18  already imported.\n",
      "Skipping  2017-09-19  already imported.\n",
      "Skipping  2017-09-20  already imported.\n",
      "Skipping  2017-09-21  already imported.\n",
      "Skipping  2017-09-22  already imported.\n",
      "Skipping  2017-09-23  already imported.\n",
      "Skipping  2017-09-24  already imported.\n",
      "Skipping  2017-09-25  already imported.\n",
      "Skipping  2017-09-26  already imported.\n",
      "Skipping  2017-09-27  already imported.\n",
      "Skipping  2017-09-28  already imported.\n",
      "Skipping  2017-09-29  already imported.\n",
      "Skipping  2017-09-30  already imported.\n",
      "Skipping  2017-10-01  already imported.\n",
      "Skipping  2017-10-02  already imported.\n",
      "Skipping  2017-10-03  already imported.\n",
      "Skipping  2017-10-04  already imported.\n",
      "Skipping  2017-10-05  already imported.\n",
      "Skipping  2017-10-06  already imported.\n",
      "Skipping  2017-10-07  already imported.\n",
      "Skipping  2017-10-08  already imported.\n",
      "Skipping  2017-10-09  already imported.\n",
      "Skipping  2017-10-10  already imported.\n",
      "Skipping  2017-10-11  already imported.\n",
      "Skipping  2017-10-12  already imported.\n",
      "Skipping  2017-10-13  already imported.\n",
      "Skipping  2017-10-14  already imported.\n",
      "Skipping  2017-10-15  already imported.\n",
      "Skipping  2017-10-16  already imported.\n",
      "Skipping  2017-10-17  already imported.\n",
      "Skipping  2017-10-18  already imported.\n",
      "Skipping  2017-10-19  already imported.\n",
      "Skipping  2017-10-20  already imported.\n",
      "Skipping  2017-10-21  already imported.\n",
      "Skipping  2017-10-22  already imported.\n",
      "Skipping  2017-10-23  already imported.\n",
      "Skipping  2017-10-24  already imported.\n",
      "Skipping  2017-10-25  already imported.\n",
      "Skipping  2017-10-26  already imported.\n",
      "Skipping  2017-10-27  already imported.\n",
      "Skipping  2017-10-28  already imported.\n",
      "Skipping  2017-10-29  already imported.\n",
      "Skipping  2017-10-30  already imported.\n",
      "Skipping  2017-10-31  already imported.\n",
      "Skipping  2017-11-01  already imported.\n",
      "Skipping  2017-11-02  already imported.\n",
      "Skipping  2017-11-03  already imported.\n",
      "Skipping  2017-11-04  already imported.\n",
      "Skipping  2017-11-05  already imported.\n",
      "Skipping  2017-11-06  already imported.\n",
      "Skipping  2017-11-07  already imported.\n",
      "Skipping  2017-11-09  already imported.\n",
      "Skipping  2017-11-10  already imported.\n",
      "Skipping  2017-11-11  already imported.\n",
      "Skipping  2017-11-12  already imported.\n",
      "Skipping  2017-11-13  already imported.\n",
      "Skipping  2017-11-14  already imported.\n",
      "Skipping  2017-11-15  already imported.\n",
      "Skipping  2017-11-16  already imported.\n",
      "Skipping  2017-11-17  already imported.\n",
      "Skipping  2017-11-18  already imported.\n",
      "Skipping  2017-11-19  already imported.\n",
      "Skipping  2017-11-20  already imported.\n",
      "Skipping  2017-11-21  already imported.\n",
      "Skipping  2017-11-22  already imported.\n",
      "Skipping  2017-11-23  already imported.\n",
      "Skipping  2017-11-24  already imported.\n",
      "Skipping  2017-11-25  already imported.\n",
      "Skipping  2017-11-26  already imported.\n",
      "Skipping  2017-11-27  already imported.\n",
      "Skipping  2017-11-28  already imported.\n",
      "Skipping  2017-11-29  already imported.\n",
      "Skipping  2017-11-30  already imported.\n",
      "Skipping  2017-12-01  already imported.\n",
      "Skipping  2017-12-02  already imported.\n",
      "Skipping  2017-12-03  already imported.\n",
      "Skipping  2017-12-04  already imported.\n",
      "Skipping  2017-12-05  already imported.\n",
      "Skipping  2017-12-06  already imported.\n",
      "Skipping  2017-12-07  already imported.\n",
      "Skipping  2017-12-08  already imported.\n",
      "Skipping  2017-12-09  already imported.\n",
      "Skipping  2017-12-10  already imported.\n",
      "Skipping  2017-12-11  already imported.\n",
      "Skipping  2017-12-12  already imported.\n",
      "Skipping  2017-12-13  already imported.\n",
      "Skipping  2017-12-14  already imported.\n",
      "Skipping  2017-12-16  already imported.\n",
      "Skipping  2017-12-17  already imported.\n",
      "Skipping  2017-12-18  already imported.\n",
      "Skipping  2017-12-19  already imported.\n",
      "Skipping  2017-12-20  already imported.\n",
      "Skipping  2017-12-21  already imported.\n",
      "Skipping  2017-12-22  already imported.\n",
      "Skipping  2017-12-24  already imported.\n",
      "Skipping  2017-12-25  already imported.\n",
      "Skipping  2017-12-26  already imported.\n",
      "Skipping  2018-02-14  already imported.\n",
      "Skipping  2018-02-15  already imported.\n",
      "Skipping  2018-02-16  already imported.\n",
      "Skipping  2018-02-17  already imported.\n",
      "Skipping  2018-02-18  already imported.\n",
      "Skipping  2018-02-19  already imported.\n",
      "Skipping  2018-02-20  already imported.\n",
      "Skipping  2018-02-21  already imported.\n",
      "Skipping  2018-02-22  already imported.\n",
      "Skipping  2018-02-23  already imported.\n",
      "Skipping  2018-02-24  already imported.\n",
      "Skipping  2018-02-25  already imported.\n",
      "Skipping  2018-02-26  already imported.\n",
      "Skipping  2018-02-27  already imported.\n",
      "Skipping  2018-02-28  already imported.\n",
      "Skipping  2018-03-01  already imported.\n",
      "Skipping  2018-03-03  already imported.\n",
      "Skipping  2018-03-04  already imported.\n",
      "Skipping  2018-03-05  already imported.\n",
      "Skipping  2018-03-06  already imported.\n",
      "Skipping  2018-03-07  already imported.\n",
      "Skipping  2018-03-08  already imported.\n",
      "Skipping  2018-03-09  already imported.\n",
      "Skipping  2018-03-10  already imported.\n",
      "Skipping  2018-03-11  already imported.\n",
      "Skipping  2018-03-12  already imported.\n",
      "Skipping  2018-03-13  already imported.\n",
      "Skipping  2018-03-14  already imported.\n",
      "Skipping  2018-03-16  already imported.\n",
      "Skipping  2018-03-17  already imported.\n",
      "Skipping  2018-03-18  already imported.\n",
      "Skipping  2018-03-19  already imported.\n",
      "Skipping  2018-03-20  already imported.\n",
      "Skipping  2018-03-21  already imported.\n",
      "Skipping  2018-03-22  already imported.\n",
      "Skipping  2018-03-23  already imported.\n",
      "Skipping  2018-03-24  already imported.\n",
      "Skipping  2018-03-25  already imported.\n",
      "Skipping  2018-03-26  already imported.\n",
      "Skipping  2018-03-27  already imported.\n",
      "Skipping  2018-03-28  already imported.\n",
      "Skipping  2018-03-29  already imported.\n",
      "Skipping  2018-03-30  already imported.\n",
      "Skipping  2018-03-31  already imported.\n",
      "Skipping  2018-04-01  already imported.\n",
      "Skipping  2018-04-02  already imported.\n",
      "Skipping  2018-04-03  already imported.\n",
      "Skipping  2018-04-04  already imported.\n",
      "Skipping  2018-04-05  already imported.\n",
      "Skipping  2018-04-06  already imported.\n",
      "Skipping  2018-04-07  already imported.\n",
      "Skipping  2018-04-08  already imported.\n",
      "Skipping  2018-04-09  already imported.\n",
      "Skipping  2018-04-10  already imported.\n",
      "Skipping  2018-04-11  already imported.\n",
      "Skipping  2018-04-12  already imported.\n",
      "Skipping  2018-04-13  already imported.\n",
      "Skipping  2018-04-14  already imported.\n",
      "Skipping  2018-04-15  already imported.\n",
      "Skipping  2018-04-16  already imported.\n",
      "Skipping  2018-04-17  already imported.\n",
      "Skipping  2018-04-18  already imported.\n",
      "Skipping  2018-04-19  already imported.\n",
      "Skipping  2018-04-20  already imported.\n",
      "Skipping  2018-04-21  already imported.\n",
      "Skipping  2018-04-22  already imported.\n",
      "Skipping  2018-04-23  already imported.\n",
      "Skipping  2018-04-24  already imported.\n",
      "Skipping  2018-04-25  already imported.\n",
      "Skipping  2018-04-26  already imported.\n",
      "Skipping  2018-04-27  already imported.\n",
      "Skipping  2018-04-28  already imported.\n",
      "Skipping  2018-04-29  already imported.\n",
      "Skipping  2018-04-30  already imported.\n",
      "Skipping  2018-05-01  already imported.\n",
      "Skipping  2018-05-02  already imported.\n",
      "Skipping  2018-05-03  already imported.\n",
      "Skipping  2018-05-04  already imported.\n",
      "Skipping  2018-05-05  already imported.\n",
      "Skipping  2018-05-06  already imported.\n",
      "Skipping  2018-05-07  already imported.\n",
      "Skipping  2018-05-08  already imported.\n",
      "Skipping  2018-05-09  already imported.\n",
      "Skipping  2018-05-10  already imported.\n",
      "Skipping  2018-05-11  already imported.\n",
      "Skipping  2018-05-12  already imported.\n",
      "Skipping  2018-05-13  already imported.\n",
      "Skipping  2018-05-14  already imported.\n",
      "Skipping  2018-05-15  already imported.\n",
      "Skipping  2018-05-16  already imported.\n",
      "Skipping  2018-05-17  already imported.\n",
      "Skipping  2018-05-18  already imported.\n",
      "Skipping  2018-05-19  already imported.\n",
      "Skipping  2018-05-20  already imported.\n",
      "Skipping  2018-05-21  already imported.\n",
      "Skipping  2018-05-22  already imported.\n",
      "Skipping  2018-05-23  already imported.\n",
      "Skipping  2018-05-24  already imported.\n",
      "Skipping  2018-05-25  already imported.\n",
      "Skipping  2018-05-26  already imported.\n",
      "Skipping  2018-05-27  already imported.\n",
      "Skipping  2018-05-28  already imported.\n",
      "Skipping  2018-05-29  already imported.\n",
      "Skipping  2018-05-30  already imported.\n",
      "Skipping  2018-05-31  already imported.\n",
      "Skipping  2018-06-01  already imported.\n",
      "Skipping  2018-06-02  already imported.\n",
      "Skipping  2018-06-03  already imported.\n",
      "Skipping  2018-06-04  already imported.\n",
      "Skipping  2018-06-05  already imported.\n",
      "Skipping  2018-06-07  already imported.\n",
      "Skipping  2018-06-08  already imported.\n",
      "Skipping  2018-06-09  already imported.\n",
      "Skipping  2018-06-10  already imported.\n",
      "Skipping  2018-06-11  already imported.\n",
      "Skipping  2018-06-13  already imported.\n",
      "Skipping  2018-06-16  already imported.\n",
      "Skipping  2018-06-17  already imported.\n",
      "Skipping  2018-06-18  already imported.\n",
      "Skipping  2018-06-19  already imported.\n",
      "Skipping  2018-06-21  already imported.\n",
      "Skipping  2018-06-23  already imported.\n",
      "Skipping  2018-06-24  already imported.\n",
      "Skipping  2018-06-25  already imported.\n",
      "Skipping  2018-06-26  already imported.\n",
      "Skipping  2018-06-27  already imported.\n",
      "Skipping  2018-06-29  already imported.\n",
      "Skipping  2018-06-30  already imported.\n",
      "Skipping  2018-07-01  already imported.\n",
      "Skipping  2018-07-03  already imported.\n",
      "Skipping  2018-07-04  already imported.\n",
      "Skipping  2018-07-05  already imported.\n",
      "Skipping  2018-07-06  already imported.\n",
      "Skipping  2018-07-07  already imported.\n",
      "Skipping  2018-07-09  already imported.\n",
      "Skipping  2018-07-10  already imported.\n",
      "Skipping  2018-07-12  already imported.\n",
      "Skipping  2018-07-13  already imported.\n",
      "Skipping  2018-07-14  already imported.\n",
      "Skipping  2018-07-15  already imported.\n",
      "Skipping  2018-07-16  already imported.\n",
      "Skipping  2018-07-17  already imported.\n",
      "Skipping  2018-07-18  already imported.\n",
      "Skipping  2018-07-19  already imported.\n",
      "Skipping  2018-07-21  already imported.\n",
      "Skipping  2018-07-22  already imported.\n",
      "Skipping  2018-07-23  already imported.\n",
      "Skipping  2018-07-24  already imported.\n",
      "Skipping  2018-07-25  already imported.\n",
      "Skipping  2018-07-26  already imported.\n",
      "Skipping  2018-07-27  already imported.\n",
      "Skipping  2018-07-28  already imported.\n",
      "Skipping  2018-07-29  already imported.\n",
      "Skipping  2018-07-30  already imported.\n",
      "Skipping  2018-07-31  already imported.\n",
      "Skipping  2018-08-01  already imported.\n",
      "Skipping  2018-08-02  already imported.\n",
      "Skipping  2018-08-03  already imported.\n",
      "Skipping  2018-08-04  already imported.\n",
      "Skipping  2018-08-05  already imported.\n",
      "Skipping  2018-08-06  already imported.\n",
      "Skipping  2018-08-07  already imported.\n",
      "Skipping  2018-08-08  already imported.\n",
      "Skipping  2018-08-09  already imported.\n",
      "Skipping  2018-08-10  already imported.\n",
      "Skipping  2018-08-11  already imported.\n",
      "Skipping  2018-08-13  already imported.\n",
      "Skipping  2018-08-15  already imported.\n",
      "Skipping  2018-08-16  already imported.\n",
      "Skipping  2018-08-17  already imported.\n",
      "Skipping  2018-08-18  already imported.\n",
      "Skipping  2018-08-19  already imported.\n",
      "Skipping  2018-08-20  already imported.\n",
      "Skipping  2018-08-21  already imported.\n",
      "Skipping  2018-08-22  already imported.\n",
      "Skipping  2018-08-23  already imported.\n",
      "Skipping  2018-08-24  already imported.\n",
      "Skipping  2018-08-25  already imported.\n",
      "Skipping  2018-08-26  already imported.\n",
      "Skipping  2018-08-27  already imported.\n",
      "Skipping  2018-08-28  already imported.\n",
      "Skipping  2018-08-29  already imported.\n",
      "Skipping  2018-08-30  already imported.\n",
      "Skipping  2018-08-31  already imported.\n",
      "Skipping  2018-09-01  already imported.\n",
      "Skipping  2018-09-02  already imported.\n",
      "Skipping  2018-09-03  already imported.\n",
      "Skipping  2018-09-04  already imported.\n",
      "Skipping  2018-09-05  already imported.\n",
      "Skipping  2018-09-06  already imported.\n",
      "Skipping  2018-09-16  already imported.\n",
      "Skipping  2018-09-17  already imported.\n",
      "Skipping  2018-09-18  already imported.\n",
      "Skipping  2018-09-19  already imported.\n",
      "Skipping  2018-09-20  already imported.\n",
      "Skipping  2018-09-21  already imported.\n",
      "Skipping  2018-09-22  already imported.\n",
      "Skipping  2018-09-23  already imported.\n",
      "Skipping  2018-09-24  already imported.\n",
      "Skipping  2018-09-25  already imported.\n",
      "Skipping  2018-09-26  already imported.\n",
      "Skipping  2018-09-27  already imported.\n",
      "Skipping  2018-09-28  already imported.\n",
      "Skipping  2018-09-29  already imported.\n",
      "Skipping  2018-09-30  already imported.\n",
      "Skipping  2018-10-01  already imported.\n",
      "Skipping  2018-10-02  already imported.\n",
      "Skipping  2018-10-03  already imported.\n",
      "Skipping  2018-10-04  already imported.\n",
      "Skipping  2018-10-05  already imported.\n",
      "Skipping  2018-10-06  already imported.\n",
      "Skipping  2018-10-07  already imported.\n",
      "Skipping  2018-10-08  already imported.\n",
      "Skipping  2018-10-09  already imported.\n",
      "Skipping  2018-10-10  already imported.\n",
      "Skipping  2018-10-11  already imported.\n",
      "Skipping  2018-10-12  already imported.\n",
      "Skipping  2018-10-13  already imported.\n",
      "Skipping  2018-10-14  already imported.\n",
      "Skipping  2018-10-15  already imported.\n",
      "Skipping  2018-10-16  already imported.\n",
      "Skipping  2018-10-17  already imported.\n",
      "Skipping  2018-10-18  already imported.\n",
      "Skipping  2018-10-19  already imported.\n",
      "Skipping  2018-10-20  already imported.\n",
      "Skipping  2018-10-21  already imported.\n",
      "Skipping  2018-10-22  already imported.\n",
      "Skipping  2018-10-23  already imported.\n",
      "(1001, 720)\n",
      "Overwrite existing file: bilinear_1001x720_304x448.nc \n",
      " You can set reuse_weights=True to save computing time.\n",
      "Saved  /home/disk/sipn/nicway/data/model/rasmesrl/forecast/sipn_nc/RASM-ESRL_2018-10-24_Stereo.nc\n",
      "(1001, 720)\n",
      "Overwrite existing file: bilinear_1001x720_304x448.nc \n",
      " You can set reuse_weights=True to save computing time.\n",
      "Saved  /home/disk/sipn/nicway/data/model/rasmesrl/forecast/sipn_nc/RASM-ESRL_2018-10-25_Stereo.nc\n",
      "(1001, 720)\n",
      "Overwrite existing file: bilinear_1001x720_304x448.nc \n",
      " You can set reuse_weights=True to save computing time.\n",
      "Saved  /home/disk/sipn/nicway/data/model/rasmesrl/forecast/sipn_nc/RASM-ESRL_2018-10-26_Stereo.nc\n",
      "(1001, 720)\n",
      "Overwrite existing file: bilinear_1001x720_304x448.nc \n",
      " You can set reuse_weights=True to save computing time.\n",
      "Saved  /home/disk/sipn/nicway/data/model/rasmesrl/forecast/sipn_nc/RASM-ESRL_2018-10-27_Stereo.nc\n",
      "(1001, 720)\n",
      "Overwrite existing file: bilinear_1001x720_304x448.nc \n",
      " You can set reuse_weights=True to save computing time.\n",
      "Saved  /home/disk/sipn/nicway/data/model/rasmesrl/forecast/sipn_nc/RASM-ESRL_2018-10-28_Stereo.nc\n",
      "(1001, 720)\n",
      "Overwrite existing file: bilinear_1001x720_304x448.nc \n",
      " You can set reuse_weights=True to save computing time.\n",
      "Saved  /home/disk/sipn/nicway/data/model/rasmesrl/forecast/sipn_nc/RASM-ESRL_2018-10-29_Stereo.nc\n"
     ]
    }
   ],
   "source": [
    "for model in all_models:\n",
    "    print('Regridding ', model, '...')\n",
    "    \n",
    "    data_dir = E.model[model][runType]['native']\n",
    "    data_out = E.model[model][runType]['sipn_nc']\n",
    "    model_grid_file = E.model[model]['grid']\n",
    "    \n",
    "    # Files are stored as per time step (about 45 per init_time)\n",
    "    # First parse files to see what unique init_times we have\n",
    "    # ARCu0.08_121_2018042112_t0300.nc\n",
    "    prefix = 'RASM-ESRL'\n",
    "    all_files = sorted(glob.glob(os.path.join(data_dir, prefix+'*.nc')))\n",
    "    # Remove init times that started on 12 our (only a few at begining of record)\n",
    "    all_files = [x for x in all_files if '-12_t' not in x]\n",
    "    init_times = list(set([s.split('_')[1].split('-00')[0] for s in all_files]))\n",
    "    \n",
    "    print(\"Found \",len(init_times),\" initialization times.\")\n",
    "    if updateall:\n",
    "        print(\"Updating all files...\")\n",
    "    else:\n",
    "        print(\"Only updating new files\")\n",
    "\n",
    "\n",
    "    weights_flag = False # Flag to set up weights have been created\n",
    "\n",
    "    # Load land/sea mask file\n",
    "    if os.path.basename(model_grid_file)!='MISSING':\n",
    "        ds_mask = xr.open_mfdataset(model_grid_file)\n",
    "    else:\n",
    "        ds_mask = None\n",
    "\n",
    "    for cf in sorted(init_times):\n",
    "        new_grid = False # Assume old grid\n",
    "        \n",
    "        # Check if already imported and skip (unless updateall flag is True)\n",
    "        f_out = os.path.join(data_out, prefix+'_'+cf+'_Stereo.nc') # netcdf file out \n",
    "        if not updateall:\n",
    "            # TODO: Test if the file is openable (not corrupted)\n",
    "            if os.path.isfile(f_out):\n",
    "                print(\"Skipping \", cf, \" already imported.\")\n",
    "                continue # Skip, file already imported\n",
    "\n",
    "        c_files = sorted(glob.glob(os.path.join(data_dir, prefix+'*_'+cf+'*.nc')))\n",
    "        ds = xr.open_mfdataset(c_files, concat_dim='time', decode_times=False, autoclose=True)\n",
    "                \n",
    "        # Fill sea ice vars (sic and hi) with zeros where there isn't any ice in ocean (previously NaNs)\n",
    "        ds_land_mask = get_land_mask_hack(ds) # get land mask from sst field\n",
    "        ds = fill_NaNOcean_with_Zeros(ds=ds, vars=['aice','hi'], \n",
    "                                      ds_land_mask=ds_land_mask)\n",
    "        \n",
    "        # Check if its the updated grid\n",
    "        if 'TLAT' not in ds:\n",
    "            new_grid = True\n",
    "\n",
    "        # Rename variables per esipn guidelines\n",
    "        if new_grid:\n",
    "            ds.rename(var_dic_new, inplace=True);\n",
    "        else:\n",
    "            ds.rename(var_dic, inplace=True);\n",
    "        \n",
    "        \n",
    "        ds = ds.drop('time_bounds')\n",
    "\n",
    "        # Format times\n",
    "        ds.coords['init_time'] = np.datetime64(cf)  #np.datetime64(ds.tau.attrs['time_origin'])\n",
    "        ds.coords['tau'] = ds.tau\n",
    "\n",
    "        ds.swap_dims({'time':'tau'}, inplace=True)\n",
    "        ds.rename({'tau':'fore_time'}, inplace=True)\n",
    "        ds.fore_time.attrs['units'] = 'Forecast offset from initial time'\n",
    "        ds = ds.drop(['time'])\n",
    "        ds.coords['fore_time'] = ds.fore_time.astype('timedelta64[h]') \n",
    "\n",
    "        # Apply masks (if available)\n",
    "        if ds_mask:\n",
    "            print('found mask')\n",
    "            # land_mask is the fraction of native grid cell that is land\n",
    "            # (1-land_mask) is fraction ocean\n",
    "            # Multiply sic by fraction ocean to get actual native grid cell sic\n",
    "            # Also mask land out where land_mask==1\n",
    "            ds = ds * (1 - ds_mask.land_mask.where(ds_mask.land_mask<1))\n",
    "\n",
    "        ds.coords['mask'] = ds.sic.isel(fore_time=0).notnull().drop(['fore_time','init_time'])\n",
    "        \n",
    "        if not new_grid:\n",
    "            # Add lat lon bounds (on fly becuase grid changes with different files (system grid change???))\n",
    "            n_j = ds.nj.size\n",
    "            n_i = ds.ni.size\n",
    "            nj_b = np.arange(0, n_j + 1)\n",
    "            ni_b = np.arange(0, n_i + 1)\n",
    "            ds_b = ds.interp(nj=nj_b-0.5, ni=ni_b-0.5, kwargs={'fill_value': None})\n",
    "\n",
    "            ds_b = ds_b.rename({'nj':'nj_b','ni':'ni_b','lat':'lat_b','lon':'lon_b'})[['lat_b','lon_b']].drop(['ULAT','ULON'])\n",
    "            ds = xr.merge([ds, ds_b])\n",
    "           \n",
    "        # Calculate regridding matrix\n",
    "        if new_grid: # Use bilinear becuase its regualar grid\n",
    "            regridder = xe.Regridder(ds, obs_grid, 'bilinear', periodic=False, reuse_weights=weights_flag)\n",
    "            weights_flag = False\n",
    "        else:   \n",
    "            regridder = xe.Regridder(ds, obs_grid, method, periodic=False, reuse_weights=weights_flag)\n",
    "            weights_flag = True # Set true for following loops\n",
    "\n",
    "        # Regrid variables\n",
    "        var_list = []\n",
    "        for cvar in ds.data_vars:\n",
    "            \n",
    "            # offset hack to keep orig missing mask\n",
    "            offset = 10.0\n",
    "            ds_coarse = regridder(ds[cvar]+offset)\n",
    "            ds_coarse = ds_coarse.where(ds_coarse!=0) - offset\n",
    "            # Bound max and min\n",
    "            if cvar=='sic':\n",
    "                c_notmissing = ds_coarse.notnull()\n",
    "                ds_coarse = ds_coarse.where(ds_coarse>=0, other=0)\n",
    "                ds_coarse = ds_coarse.where(ds_coarse<=1, other=1)\n",
    "                ds_coarse = ds_coarse.where(c_notmissing)\n",
    "            elif cvar=='hi':\n",
    "                c_notmissing = ds_coarse.notnull()\n",
    "                ds_coarse = ds_coarse.where(ds_coarse>=0, other=0)\n",
    "                ds_coarse = ds_coarse.where(c_notmissing)\n",
    "                \n",
    "            var_list.append(ds_coarse)\n",
    "        ds_out = xr.merge(var_list)\n",
    "\n",
    "        # Expand dims\n",
    "        ds_out = import_data.expand_to_sipn_dims(ds_out)\n",
    "        \n",
    "#         plt.figure(figsize=(12*400/300,12))\n",
    "#         ds_out.sic[0,0,0,:,:].plot()\n",
    "#         print(ds.sic.max().values)\n",
    "#         print(ds.sic.min().values)\n",
    "#         print(ds_out.sic.max().values)\n",
    "#         print(ds_out.sic.min().values)\n",
    "\n",
    "#         # Save regridded to netcdf file\n",
    "#         xr.exit()\n",
    "        \n",
    "        ds_out.to_netcdf(f_out)\n",
    "        ds_out = None # Memory clean up\n",
    "        ds = None\n",
    "        print('Saved ', f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regridder = xe.Regridder(ds, obs_grid, method, periodic=True, reuse_weights=weights_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.plot(ds.lon.values.flatten(), ds.lat.values.flatten(),'ro',label='center')\n",
    "# plt.plot(ds.lon_b.values.flatten(), ds.lat_b.values.flatten(),'ko',label='bounds')\n",
    "\n",
    "\n",
    "# plt.plot(obs_grid.lon.values.flatten(), obs_grid.lat.values.flatten(),'mo',\n",
    "#          label='center_OBS')\n",
    "# plt.plot(obs_grid.lon_b.values.flatten(), obs_grid.lat_b.values.flatten(),'go',\n",
    "#          label='center_OBS')\n",
    "# plt.plot(ds.lon.values.flatten(), ds.lat.values.flatten(),'ro',label='center')\n",
    "# plt.plot(ds.lon_b.values.flatten(), ds.lat_b.values.flatten(),'ko',label='bounds')\n",
    "\n",
    "\n",
    "# (f, ax1) = ice_plot.polar_axis()\n",
    "# plt.plot(obs_grid.lon.values.flatten(), obs_grid.lat.values.flatten(),'mo',\n",
    "#          label='center_OBS', transform=ccrs.PlateCarree())\n",
    "# # plt.plot(obs_grid.lon_b.values[0:10,0:10].flatten(), obs_grid.lat_b[0:10,0:10].values.flatten(),'go',\n",
    "# #          label='center_OBS')\n",
    "# # plt.plot(ds.lon.values.flatten(), ds.lat.values.flatten(),'ro',label='center')\n",
    "# ax1.plot(ds.lon_b.values.flatten(), ds.lat_b.values.flatten(),'ko',label='bounds',\n",
    "#         transform=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ds.sic.isel(fore_time=0).notnull().drop(['fore_time','init_time']).plot()\n",
    "\n",
    "# ds.mask.plot()\n",
    "\n",
    "# ds_out\n",
    "\n",
    "# plt.figure(figsize=(12*400/300,12))\n",
    "# ds_out.mask[:,:].plot()\n",
    "\n",
    "# plt.figure(figsize=(12*400/300,12))\n",
    "# ds.sic[0,:,:].plot()\n",
    "# print(ds.sic[0,:,:].mean().values)\n",
    "\n",
    "# plt.figure(figsize=(12*400/300,12))\n",
    "# ds_out.sic[0,0,0,:,:].plot(vmin=0, vmax=1)\n",
    "# print(ds_out.sic[0,:,:].mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if weights_flag:\n",
    "    regridder.clean_weight_file()  # clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sic_all = xr.open_mfdataset(f_out)\n",
    "\n",
    "# # Set up plotting info\n",
    "# cmap_sic = matplotlib.colors.ListedColormap(sns.color_palette(\"Blues\", 10))\n",
    "# cmap_sic.set_bad(color = 'red')\n",
    "\n",
    "# # Plot original projection\n",
    "# plt.figure(figsize=(20,10))\n",
    "# ax1 = plt.axes(projection=ccrs.PlateCarree())\n",
    "# ds_p = ds.sic.isel(fore_time=8)\n",
    "# ds_p.plot.pcolormesh(ax=ax1, x='lon', y='lat', \n",
    "#                                  vmin=0, vmax=1,\n",
    "#                                  cmap=matplotlib.colors.ListedColormap(sns.color_palette(\"Blues\", 10)),\n",
    "#                     transform=ccrs.PlateCarree());\n",
    "# ax1.set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())\n",
    "# gl = ax1.gridlines(crs=ccrs.PlateCarree(), linestyle='-')\n",
    "# gl.xlabels_bottom = True\n",
    "# gl.ylabels_left = True\n",
    "# gl.xformatter = LONGITUDE_FORMATTER\n",
    "# gl.yformatter = LATITUDE_FORMATTER\n",
    "# ax1.coastlines(linewidth=0.75, color='black', resolution='50m');\n",
    "\n",
    "# # Plot SIC on target projection\n",
    "# (f, ax1) = ice_plot.polar_axis()\n",
    "# ds_p.plot.pcolormesh(ax=ax1, x='lon', y='lat', \n",
    "#                                      transform=ccrs.PlateCarree(),\n",
    "#                                      cmap=cmap_sic)\n",
    "# ax1.set_title('Original Grid')\n",
    "\n",
    "# # Plot SIC on target projection\n",
    "# (f, ax1) = ice_plot.polar_axis()\n",
    "# ds_p2 = sic_all.sic.isel(init_time=0).isel(fore_time=8).isel(ensemble=0)\n",
    "# ds_p2.plot.pcolormesh(ax=ax1, x='lon', y='lat', \n",
    "#                                      transform=ccrs.PlateCarree(),\n",
    "#                                      cmap=cmap_sic)\n",
    "# ax1.set_title('Target Grid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.4 esio",
   "language": "python",
   "name": "esio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
